{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Device Name: /physical_device:GPU:0\n",
      "Device Type: GPU\n",
      "Device Details: {'device_name': 'Quadro P2000', 'compute_capability': (6, 1)}\n",
      "CUDA Version: 64_112\n",
      "cuDNN Version: 64_8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Kiểm tra số lượng GPU có sẵn\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "# Kiểm tra chi tiết các GPU\n",
    "for gpu in gpus:\n",
    "    print(f\"Device Name: {gpu.name}\")\n",
    "    print(f\"Device Type: {gpu.device_type}\")\n",
    "\n",
    "    # Lấy và in thông tin bộ nhớ GPU (sử dụng phương pháp khác nếu cần)\n",
    "    try:\n",
    "        # In thông tin chi tiết về thiết bị\n",
    "        device_details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device Details: {device_details}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get detailed info for {gpu.name}: {e}\")\n",
    "\n",
    "# Kiểm tra khả năng tương thích CUDA và cuDNN\n",
    "try:\n",
    "    from tensorflow.python.framework import ops\n",
    "    build_info = tf.sysconfig.get_build_info()\n",
    "    print(\"CUDA Version:\", build_info['cuda_version'])\n",
    "    print(\"cuDNN Version:\", build_info['cudnn_version'])\n",
    "except Exception as e:\n",
    "    print(f\"Could not get CUDA and cuDNN versions: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir = \"Rice_photos\"\n",
    "folder_dims = {}\n",
    "\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    if files:\n",
    "        dims = []\n",
    "        for file in files:\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            if img_path.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(img_path)\n",
    "                dims.append(img.size)  # img.size returns (width, height)\n",
    "\n",
    "        # Calculate the average (x, y) dimensions for the current folder\n",
    "        if dims:\n",
    "            avg_dims = np.mean(dims, axis=0)\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_dims[folder_name] = avg_dims\n",
    "\n",
    "folders = list(folder_dims.keys())\n",
    "avg_widths = [dims[0] for dims in folder_dims.values()]\n",
    "avg_heights = [dims[1] for dims in folder_dims.values()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35 \n",
    "\n",
    "# Bar chart for width and height\n",
    "ax.bar(np.arange(len(folders)) - width/2, avg_widths, width, label='Width')\n",
    "ax.bar(np.arange(len(folders)) + width/2, avg_heights, width, label='Height')\n",
    "ax.set_ylabel('Average Dimension (pixels)')\n",
    "ax.set_title('Average Image Dimensions by Folder')\n",
    "ax.set_xticks(np.arange(len(folders)))\n",
    "ax.set_xticklabels(folders, rotation=45)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BC-15': array([222.76281352,  72.77589967]), 'Negative_BC-15': array([224.35214324,  75.33152469]), 'Huong_thom-1': array([233.59494275,  73.64742366]), 'Negative_Huong_thom-1': array([220.9211295,  77.2375852]), 'Negative_Nep-87': array([229.93080054,  73.1431479 ]), 'Nep-87': array([197.57469621,  88.3659757 ]), 'Negative_Q-5_modify': array([226.95866667,  74.87066667]), 'Q-5_modify': array([212.78543046,  79.18476821]), 'Negative_Thien_uu-8': array([219.87715736,  78.07106599]), 'Thien_uu-8': array([243.92164545,  63.81978452]), 'Negative_Xi-23': array([221.38216893,  76.12930136]), 'Xi-23': array([228.89312977,  78.01706331])}\n"
     ]
    }
   ],
   "source": [
    "print(folder_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset đã được chia thành công!\n"
     ]
    }
   ],
   "source": [
    "# original_dataset_dir = \"./Rice_photos/Xi23\"\n",
    "\n",
    "# base_dir = 'D:/DPL302m/split_Xi23'\n",
    "# train_dir = os.path.join(base_dir, 'train')\n",
    "# validation_dir = os.path.join(base_dir, 'validation')\n",
    "# test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# os.makedirs(train_dir, exist_ok=True)\n",
    "# os.makedirs(validation_dir, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# for category in os.listdir(original_dataset_dir):\n",
    "#     category_path = os.path.join(original_dataset_dir, category)\n",
    "    \n",
    "#     if os.path.isdir(category_path):\n",
    "#         images = os.listdir(category_path)\n",
    "#         train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "#         train_images, val_images = train_test_split(train_images, test_size=0.25, random_state=42)  # 0.25 * 80% = 20%\n",
    "        \n",
    "#         os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(validation_dir, category), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
    "\n",
    "#         for image in train_images:\n",
    "#             src = os.path.join(category_path, image)\n",
    "#             dst = os.path.join(train_dir, category, image)\n",
    "#             shutil.copyfile(src, dst)\n",
    "\n",
    "#         for image in val_images:\n",
    "#             src = os.path.join(category_path, image)\n",
    "#             dst = os.path.join(validation_dir, category, image)\n",
    "#             shutil.copyfile(src, dst)\n",
    "\n",
    "#         for image in test_images:\n",
    "#             src = os.path.join(category_path, image)\n",
    "#             dst = os.path.join(test_dir, category, image)\n",
    "#             shutil.copyfile(src, dst)\n",
    "\n",
    "# print(\"Dataset đã được chia thành công!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2204 images belonging to 2 classes.\n",
      "Found 737 images belonging to 2 classes.\n",
      "Found 736 images belonging to 2 classes.\n",
      "Found 2488 images belonging to 2 classes.\n",
      "Found 830 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n",
      "Found 1722 images belonging to 2 classes.\n",
      "Found 574 images belonging to 2 classes.\n",
      "Found 577 images belonging to 2 classes.\n",
      "Found 1803 images belonging to 2 classes.\n",
      "Found 604 images belonging to 2 classes.\n",
      "Found 603 images belonging to 2 classes.\n",
      "Found 1203 images belonging to 2 classes.\n",
      "Found 401 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "Found 2485 images belonging to 2 classes.\n",
      "Found 830 images belonging to 2 classes.\n",
      "Found 830 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#BC-15\n",
    "train_generator_BC15 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_BC-15/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_BC15 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_BC-15/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_BC15 = test_datagen.flow_from_directory(\n",
    "    \"./data/split_BC-15/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "# Huongthom\n",
    "train_generator_Huongthom = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Huongthom/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_Huongthom = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Huongthom/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_Huongthom = test_datagen.flow_from_directory(\n",
    "    \"./data/split_Huongthom/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Nep87\n",
    "train_generator_Nep87 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Nep87/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_Nep87 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Nep87/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_Nep87 = test_datagen.flow_from_directory(\n",
    "    \"./data/split_Nep87/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Q5\n",
    "train_generator_Q5 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Q5/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_Q5 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Q5/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_Q5 = test_datagen.flow_from_directory(\n",
    "    \"./data/split_Q5/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Thien_uu\n",
    "train_generator_Thien_uu = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Thien_uu/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_Thien_uu = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Thien_uu/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_Thien_uu = test_datagen.flow_from_directory(\n",
    "    \"./data/split_Thien_uu/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Xi23\n",
    "train_generator_Xi23 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Xi23/train\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator_Xi23 = train_datagen.flow_from_directory(\n",
    "    \"./data/split_Xi23/validation\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator_Xi23 = test_datagen.flow_from_directory(\n",
    "    \"./data/split_Xi23/test\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 83.8M    0 80655    0     0   103k      0  0:13:51 --:--:--  0:13:51  103k\n",
      "  0 83.8M    0  808k    0     0   593k      0  0:02:24  0:00:01  0:02:23  594k\n",
      "  9 83.8M    9 8061k    0     0  3401k      0  0:00:25  0:00:02  0:00:23 3405k\n",
      " 17 83.8M   17 14.3M    0     0  4374k      0  0:00:19  0:00:03  0:00:16 4377k\n",
      " 26 83.8M   26 21.8M    0     0  5127k      0  0:00:16  0:00:04  0:00:12 5131k\n",
      " 33 83.8M   33 28.0M    0     0  5355k      0  0:00:16  0:00:05  0:00:11 6225k\n",
      " 39 83.8M   39 33.4M    0     0  5379k      0  0:00:15  0:00:06  0:00:09 6681k\n",
      " 47 83.8M   47 39.9M    0     0  5547k      0  0:00:15  0:00:07  0:00:08 6564k\n",
      " 53 83.8M   53 45.1M    0     0  5522k      0  0:00:15  0:00:08  0:00:07 6294k\n",
      " 62 83.8M   62 52.2M    0     0  5714k      0  0:00:15  0:00:09  0:00:06 6226k\n",
      " 70 83.8M   70 59.4M    0     0  5874k      0  0:00:14  0:00:10  0:00:04 6431k\n",
      " 78 83.8M   78 65.5M    0     0  5905k      0  0:00:14  0:00:11  0:00:03 6575k\n",
      " 84 83.8M   84 70.4M    0     0  5837k      0  0:00:14  0:00:12  0:00:02 6265k\n",
      " 89 83.8M   89 75.3M    0     0  5770k      0  0:00:14  0:00:13  0:00:01 6185k\n",
      " 96 83.8M   96 81.0M    0     0  5776k      0  0:00:14  0:00:14 --:--:-- 5891k\n",
      "100 83.8M  100 83.8M    0     0  5756k      0  0:00:14  0:00:14 --:--:-- 5487k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ./inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_564 (Conv2D)            (None, 127, 127, 32  864         ['input_7[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_574 (Batch  (None, 127, 127, 32  96         ['conv2d_564[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_564 (Activation)    (None, 127, 127, 32  0           ['batch_normalization_574[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_565 (Conv2D)            (None, 125, 125, 32  9216        ['activation_564[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_575 (Batch  (None, 125, 125, 32  96         ['conv2d_565[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_565 (Activation)    (None, 125, 125, 32  0           ['batch_normalization_575[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_566 (Conv2D)            (None, 125, 125, 64  18432       ['activation_565[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_576 (Batch  (None, 125, 125, 64  192        ['conv2d_566[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_566 (Activation)    (None, 125, 125, 64  0           ['batch_normalization_576[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_24 (MaxPooling2D  (None, 62, 62, 64)  0           ['activation_566[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_567 (Conv2D)            (None, 62, 62, 80)   5120        ['max_pooling2d_24[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_577 (Batch  (None, 62, 62, 80)  240         ['conv2d_567[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_567 (Activation)    (None, 62, 62, 80)   0           ['batch_normalization_577[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_568 (Conv2D)            (None, 60, 60, 192)  138240      ['activation_567[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_578 (Batch  (None, 60, 60, 192)  576        ['conv2d_568[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_568 (Activation)    (None, 60, 60, 192)  0           ['batch_normalization_578[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_25 (MaxPooling2D  (None, 29, 29, 192)  0          ['activation_568[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_572 (Conv2D)            (None, 29, 29, 64)   12288       ['max_pooling2d_25[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_582 (Batch  (None, 29, 29, 64)  192         ['conv2d_572[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_572 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_582[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_570 (Conv2D)            (None, 29, 29, 48)   9216        ['max_pooling2d_25[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_573 (Conv2D)            (None, 29, 29, 96)   55296       ['activation_572[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_580 (Batch  (None, 29, 29, 48)  144         ['conv2d_570[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_583 (Batch  (None, 29, 29, 96)  288         ['conv2d_573[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_570 (Activation)    (None, 29, 29, 48)   0           ['batch_normalization_580[0][0]']\n",
      "                                                                                                  \n",
      " activation_573 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_583[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_54 (AverageP  (None, 29, 29, 192)  0          ['max_pooling2d_25[0][0]']       \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_569 (Conv2D)            (None, 29, 29, 64)   12288       ['max_pooling2d_25[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_571 (Conv2D)            (None, 29, 29, 64)   76800       ['activation_570[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_574 (Conv2D)            (None, 29, 29, 96)   82944       ['activation_573[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_575 (Conv2D)            (None, 29, 29, 32)   6144        ['average_pooling2d_54[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_579 (Batch  (None, 29, 29, 64)  192         ['conv2d_569[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_581 (Batch  (None, 29, 29, 64)  192         ['conv2d_571[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_584 (Batch  (None, 29, 29, 96)  288         ['conv2d_574[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_585 (Batch  (None, 29, 29, 32)  96          ['conv2d_575[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_569 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_579[0][0]']\n",
      "                                                                                                  \n",
      " activation_571 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_581[0][0]']\n",
      "                                                                                                  \n",
      " activation_574 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_584[0][0]']\n",
      "                                                                                                  \n",
      " activation_575 (Activation)    (None, 29, 29, 32)   0           ['batch_normalization_585[0][0]']\n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 29, 29, 256)  0           ['activation_569[0][0]',         \n",
      "                                                                  'activation_571[0][0]',         \n",
      "                                                                  'activation_574[0][0]',         \n",
      "                                                                  'activation_575[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_579 (Conv2D)            (None, 29, 29, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_589 (Batch  (None, 29, 29, 64)  192         ['conv2d_579[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_579 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_589[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_577 (Conv2D)            (None, 29, 29, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_580 (Conv2D)            (None, 29, 29, 96)   55296       ['activation_579[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_587 (Batch  (None, 29, 29, 48)  144         ['conv2d_577[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_590 (Batch  (None, 29, 29, 96)  288         ['conv2d_580[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_577 (Activation)    (None, 29, 29, 48)   0           ['batch_normalization_587[0][0]']\n",
      "                                                                                                  \n",
      " activation_580 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_590[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_55 (AverageP  (None, 29, 29, 256)  0          ['mixed0[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_576 (Conv2D)            (None, 29, 29, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_578 (Conv2D)            (None, 29, 29, 64)   76800       ['activation_577[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_581 (Conv2D)            (None, 29, 29, 96)   82944       ['activation_580[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_582 (Conv2D)            (None, 29, 29, 64)   16384       ['average_pooling2d_55[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_586 (Batch  (None, 29, 29, 64)  192         ['conv2d_576[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_588 (Batch  (None, 29, 29, 64)  192         ['conv2d_578[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_591 (Batch  (None, 29, 29, 96)  288         ['conv2d_581[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_592 (Batch  (None, 29, 29, 64)  192         ['conv2d_582[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_576 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_586[0][0]']\n",
      "                                                                                                  \n",
      " activation_578 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_588[0][0]']\n",
      "                                                                                                  \n",
      " activation_581 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_591[0][0]']\n",
      "                                                                                                  \n",
      " activation_582 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_592[0][0]']\n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 29, 29, 288)  0           ['activation_576[0][0]',         \n",
      "                                                                  'activation_578[0][0]',         \n",
      "                                                                  'activation_581[0][0]',         \n",
      "                                                                  'activation_582[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_586 (Conv2D)            (None, 29, 29, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_596 (Batch  (None, 29, 29, 64)  192         ['conv2d_586[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_586 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_596[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_584 (Conv2D)            (None, 29, 29, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_587 (Conv2D)            (None, 29, 29, 96)   55296       ['activation_586[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_594 (Batch  (None, 29, 29, 48)  144         ['conv2d_584[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_597 (Batch  (None, 29, 29, 96)  288         ['conv2d_587[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_584 (Activation)    (None, 29, 29, 48)   0           ['batch_normalization_594[0][0]']\n",
      "                                                                                                  \n",
      " activation_587 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_597[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_56 (AverageP  (None, 29, 29, 288)  0          ['mixed1[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_583 (Conv2D)            (None, 29, 29, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_585 (Conv2D)            (None, 29, 29, 64)   76800       ['activation_584[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_588 (Conv2D)            (None, 29, 29, 96)   82944       ['activation_587[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_589 (Conv2D)            (None, 29, 29, 64)   18432       ['average_pooling2d_56[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_593 (Batch  (None, 29, 29, 64)  192         ['conv2d_583[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_595 (Batch  (None, 29, 29, 64)  192         ['conv2d_585[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_598 (Batch  (None, 29, 29, 96)  288         ['conv2d_588[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_599 (Batch  (None, 29, 29, 64)  192         ['conv2d_589[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_583 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_593[0][0]']\n",
      "                                                                                                  \n",
      " activation_585 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_595[0][0]']\n",
      "                                                                                                  \n",
      " activation_588 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_598[0][0]']\n",
      "                                                                                                  \n",
      " activation_589 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_599[0][0]']\n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 29, 29, 288)  0           ['activation_583[0][0]',         \n",
      "                                                                  'activation_585[0][0]',         \n",
      "                                                                  'activation_588[0][0]',         \n",
      "                                                                  'activation_589[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_591 (Conv2D)            (None, 29, 29, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_601 (Batch  (None, 29, 29, 64)  192         ['conv2d_591[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_591 (Activation)    (None, 29, 29, 64)   0           ['batch_normalization_601[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_592 (Conv2D)            (None, 29, 29, 96)   55296       ['activation_591[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_602 (Batch  (None, 29, 29, 96)  288         ['conv2d_592[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_592 (Activation)    (None, 29, 29, 96)   0           ['batch_normalization_602[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_590 (Conv2D)            (None, 14, 14, 384)  995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_593 (Conv2D)            (None, 14, 14, 96)   82944       ['activation_592[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_600 (Batch  (None, 14, 14, 384)  1152       ['conv2d_590[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_603 (Batch  (None, 14, 14, 96)  288         ['conv2d_593[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_590 (Activation)    (None, 14, 14, 384)  0           ['batch_normalization_600[0][0]']\n",
      "                                                                                                  \n",
      " activation_593 (Activation)    (None, 14, 14, 96)   0           ['batch_normalization_603[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_26 (MaxPooling2D  (None, 14, 14, 288)  0          ['mixed2[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 14, 14, 768)  0           ['activation_590[0][0]',         \n",
      "                                                                  'activation_593[0][0]',         \n",
      "                                                                  'max_pooling2d_26[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_598 (Conv2D)            (None, 14, 14, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_608 (Batch  (None, 14, 14, 128)  384        ['conv2d_598[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_598 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_608[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_599 (Conv2D)            (None, 14, 14, 128)  114688      ['activation_598[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_609 (Batch  (None, 14, 14, 128)  384        ['conv2d_599[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_599 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_609[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_595 (Conv2D)            (None, 14, 14, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_600 (Conv2D)            (None, 14, 14, 128)  114688      ['activation_599[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_605 (Batch  (None, 14, 14, 128)  384        ['conv2d_595[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_610 (Batch  (None, 14, 14, 128)  384        ['conv2d_600[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_595 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_605[0][0]']\n",
      "                                                                                                  \n",
      " activation_600 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_610[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_596 (Conv2D)            (None, 14, 14, 128)  114688      ['activation_595[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_601 (Conv2D)            (None, 14, 14, 128)  114688      ['activation_600[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_606 (Batch  (None, 14, 14, 128)  384        ['conv2d_596[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_611 (Batch  (None, 14, 14, 128)  384        ['conv2d_601[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_596 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_606[0][0]']\n",
      "                                                                                                  \n",
      " activation_601 (Activation)    (None, 14, 14, 128)  0           ['batch_normalization_611[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_57 (AverageP  (None, 14, 14, 768)  0          ['mixed3[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_594 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_597 (Conv2D)            (None, 14, 14, 192)  172032      ['activation_596[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_602 (Conv2D)            (None, 14, 14, 192)  172032      ['activation_601[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_603 (Conv2D)            (None, 14, 14, 192)  147456      ['average_pooling2d_57[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_604 (Batch  (None, 14, 14, 192)  576        ['conv2d_594[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_607 (Batch  (None, 14, 14, 192)  576        ['conv2d_597[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_612 (Batch  (None, 14, 14, 192)  576        ['conv2d_602[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_613 (Batch  (None, 14, 14, 192)  576        ['conv2d_603[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_594 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_604[0][0]']\n",
      "                                                                                                  \n",
      " activation_597 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_607[0][0]']\n",
      "                                                                                                  \n",
      " activation_602 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_612[0][0]']\n",
      "                                                                                                  \n",
      " activation_603 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_613[0][0]']\n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 14, 14, 768)  0           ['activation_594[0][0]',         \n",
      "                                                                  'activation_597[0][0]',         \n",
      "                                                                  'activation_602[0][0]',         \n",
      "                                                                  'activation_603[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_608 (Conv2D)            (None, 14, 14, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_618 (Batch  (None, 14, 14, 160)  480        ['conv2d_608[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_608 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_618[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_609 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_608[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_619 (Batch  (None, 14, 14, 160)  480        ['conv2d_609[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_609 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_619[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_605 (Conv2D)            (None, 14, 14, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_610 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_609[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_615 (Batch  (None, 14, 14, 160)  480        ['conv2d_605[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_620 (Batch  (None, 14, 14, 160)  480        ['conv2d_610[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_605 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_615[0][0]']\n",
      "                                                                                                  \n",
      " activation_610 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_620[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_606 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_605[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_611 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_610[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_616 (Batch  (None, 14, 14, 160)  480        ['conv2d_606[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_621 (Batch  (None, 14, 14, 160)  480        ['conv2d_611[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_606 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_616[0][0]']\n",
      "                                                                                                  \n",
      " activation_611 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_621[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_58 (AverageP  (None, 14, 14, 768)  0          ['mixed4[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_604 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_607 (Conv2D)            (None, 14, 14, 192)  215040      ['activation_606[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_612 (Conv2D)            (None, 14, 14, 192)  215040      ['activation_611[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_613 (Conv2D)            (None, 14, 14, 192)  147456      ['average_pooling2d_58[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_614 (Batch  (None, 14, 14, 192)  576        ['conv2d_604[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_617 (Batch  (None, 14, 14, 192)  576        ['conv2d_607[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_622 (Batch  (None, 14, 14, 192)  576        ['conv2d_612[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_623 (Batch  (None, 14, 14, 192)  576        ['conv2d_613[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_604 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_614[0][0]']\n",
      "                                                                                                  \n",
      " activation_607 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_617[0][0]']\n",
      "                                                                                                  \n",
      " activation_612 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_622[0][0]']\n",
      "                                                                                                  \n",
      " activation_613 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_623[0][0]']\n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 14, 14, 768)  0           ['activation_604[0][0]',         \n",
      "                                                                  'activation_607[0][0]',         \n",
      "                                                                  'activation_612[0][0]',         \n",
      "                                                                  'activation_613[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_618 (Conv2D)            (None, 14, 14, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_628 (Batch  (None, 14, 14, 160)  480        ['conv2d_618[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_618 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_628[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_619 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_618[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_629 (Batch  (None, 14, 14, 160)  480        ['conv2d_619[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_619 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_629[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_615 (Conv2D)            (None, 14, 14, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_620 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_619[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_625 (Batch  (None, 14, 14, 160)  480        ['conv2d_615[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_630 (Batch  (None, 14, 14, 160)  480        ['conv2d_620[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_615 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_625[0][0]']\n",
      "                                                                                                  \n",
      " activation_620 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_630[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_616 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_615[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_621 (Conv2D)            (None, 14, 14, 160)  179200      ['activation_620[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_626 (Batch  (None, 14, 14, 160)  480        ['conv2d_616[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_631 (Batch  (None, 14, 14, 160)  480        ['conv2d_621[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_616 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_626[0][0]']\n",
      "                                                                                                  \n",
      " activation_621 (Activation)    (None, 14, 14, 160)  0           ['batch_normalization_631[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_59 (AverageP  (None, 14, 14, 768)  0          ['mixed5[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_614 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_617 (Conv2D)            (None, 14, 14, 192)  215040      ['activation_616[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_622 (Conv2D)            (None, 14, 14, 192)  215040      ['activation_621[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_623 (Conv2D)            (None, 14, 14, 192)  147456      ['average_pooling2d_59[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_624 (Batch  (None, 14, 14, 192)  576        ['conv2d_614[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_627 (Batch  (None, 14, 14, 192)  576        ['conv2d_617[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_632 (Batch  (None, 14, 14, 192)  576        ['conv2d_622[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_633 (Batch  (None, 14, 14, 192)  576        ['conv2d_623[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_614 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_624[0][0]']\n",
      "                                                                                                  \n",
      " activation_617 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_627[0][0]']\n",
      "                                                                                                  \n",
      " activation_622 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_632[0][0]']\n",
      "                                                                                                  \n",
      " activation_623 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_633[0][0]']\n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 14, 14, 768)  0           ['activation_614[0][0]',         \n",
      "                                                                  'activation_617[0][0]',         \n",
      "                                                                  'activation_622[0][0]',         \n",
      "                                                                  'activation_623[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_628 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_638 (Batch  (None, 14, 14, 192)  576        ['conv2d_628[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_628 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_638[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_629 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_628[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_639 (Batch  (None, 14, 14, 192)  576        ['conv2d_629[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_629 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_639[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_625 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_630 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_629[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_635 (Batch  (None, 14, 14, 192)  576        ['conv2d_625[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_640 (Batch  (None, 14, 14, 192)  576        ['conv2d_630[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_625 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_635[0][0]']\n",
      "                                                                                                  \n",
      " activation_630 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_640[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_626 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_625[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_631 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_630[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_636 (Batch  (None, 14, 14, 192)  576        ['conv2d_626[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_641 (Batch  (None, 14, 14, 192)  576        ['conv2d_631[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_626 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_636[0][0]']\n",
      "                                                                                                  \n",
      " activation_631 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_641[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_60 (AverageP  (None, 14, 14, 768)  0          ['mixed6[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_624 (Conv2D)            (None, 14, 14, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_627 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_626[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_632 (Conv2D)            (None, 14, 14, 192)  258048      ['activation_631[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_633 (Conv2D)            (None, 14, 14, 192)  147456      ['average_pooling2d_60[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_634 (Batch  (None, 14, 14, 192)  576        ['conv2d_624[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_637 (Batch  (None, 14, 14, 192)  576        ['conv2d_627[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_642 (Batch  (None, 14, 14, 192)  576        ['conv2d_632[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_643 (Batch  (None, 14, 14, 192)  576        ['conv2d_633[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_624 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_634[0][0]']\n",
      "                                                                                                  \n",
      " activation_627 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_637[0][0]']\n",
      "                                                                                                  \n",
      " activation_632 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_642[0][0]']\n",
      "                                                                                                  \n",
      " activation_633 (Activation)    (None, 14, 14, 192)  0           ['batch_normalization_643[0][0]']\n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 14, 14, 768)  0           ['activation_624[0][0]',         \n",
      "                                                                  'activation_627[0][0]',         \n",
      "                                                                  'activation_632[0][0]',         \n",
      "                                                                  'activation_633[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 150528)       0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 128)          19267712    ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_668 (Batch  (None, 128)         512         ['dense_16[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 128)          0           ['batch_normalization_668[0][0]']\n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 64)           8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_669 (Batch  (None, 64)          256         ['dense_17[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 64)           0           ['batch_normalization_669[0][0]']\n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1)            65          ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,252,065\n",
      "Trainable params: 28,232,865\n",
      "Non-trainable params: 19,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "local_weights_file = './models_save/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "pre_trained_model = InceptionV3(input_shape = (256, 256, 3),  \n",
    "                                include_top = False,\n",
    "                                weights = None) \n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output\n",
    "\n",
    "\n",
    "X = layers.Flatten()(last_output)\n",
    "X = layers.Dense(128, activation='relu')(X)  \n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X) \n",
    "X = layers.Dense(64, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(1, activation='sigmoid')(X)\n",
    "model = Model(pre_trained_model.input, X)\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.0001),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 5, restore_best_weights=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "137/137 [==============================] - 62s 411ms/step - loss: 0.5501 - accuracy: 0.7463 - val_loss: 0.2663 - val_accuracy: 0.8913\n",
      "Epoch 2/20\n",
      "137/137 [==============================] - 62s 454ms/step - loss: 0.2008 - accuracy: 0.9250 - val_loss: 0.3663 - val_accuracy: 0.8302\n",
      "Epoch 3/20\n",
      "137/137 [==============================] - 60s 441ms/step - loss: 0.0968 - accuracy: 0.9726 - val_loss: 0.1273 - val_accuracy: 0.9565\n",
      "Epoch 4/20\n",
      "137/137 [==============================] - 65s 476ms/step - loss: 0.1016 - accuracy: 0.9712 - val_loss: 0.0828 - val_accuracy: 0.9701\n",
      "Epoch 5/20\n",
      "137/137 [==============================] - 70s 508ms/step - loss: 0.0834 - accuracy: 0.9776 - val_loss: 0.0901 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "137/137 [==============================] - 65s 475ms/step - loss: 0.0489 - accuracy: 0.9877 - val_loss: 0.0957 - val_accuracy: 0.9633\n",
      "Epoch 7/20\n",
      "137/137 [==============================] - 66s 479ms/step - loss: 0.0623 - accuracy: 0.9822 - val_loss: 0.0916 - val_accuracy: 0.9660\n",
      "Epoch 8/20\n",
      "137/137 [==============================] - 66s 480ms/step - loss: 0.0507 - accuracy: 0.9877 - val_loss: 0.0706 - val_accuracy: 0.9715\n",
      "Epoch 9/20\n",
      "137/137 [==============================] - 61s 446ms/step - loss: 0.0383 - accuracy: 0.9904 - val_loss: 0.1403 - val_accuracy: 0.9511\n",
      "Epoch 10/20\n",
      "137/137 [==============================] - 62s 452ms/step - loss: 0.0553 - accuracy: 0.9813 - val_loss: 0.0881 - val_accuracy: 0.9688\n",
      "Epoch 11/20\n",
      "137/137 [==============================] - 62s 450ms/step - loss: 0.0490 - accuracy: 0.9863 - val_loss: 0.1194 - val_accuracy: 0.9633\n",
      "Epoch 12/20\n",
      "137/137 [==============================] - 61s 443ms/step - loss: 0.0738 - accuracy: 0.9785 - val_loss: 0.1492 - val_accuracy: 0.9497\n",
      "Epoch 13/20\n",
      "137/137 [==============================] - 62s 451ms/step - loss: 0.0634 - accuracy: 0.9808 - val_loss: 1.0739 - val_accuracy: 0.7378\n",
      "Training Time: 822.8871421813965 seconds\n",
      "46/46 [==============================] - 9s 202ms/step - loss: 0.0840 - accuracy: 0.9701\n",
      "BC-15 - Test Loss: 0.0839909166097641\n",
      "BC-15 - Test Accuracy: 0.970108687877655\n"
     ]
    }
   ],
   "source": [
    "#BC15\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_BC15,\n",
    "    steps_per_epoch=train_generator_BC15.samples // train_generator_BC15.batch_size,\n",
    "    validation_data=validation_generator_BC15,\n",
    "    validation_steps=validation_generator_BC15.samples // validation_generator_BC15.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "#30p21\n",
    "test_loss_BC15, test_accuracy_BC15 = model.evaluate(test_generator_BC15)\n",
    "print(f'BC-15 - Test Loss: {test_loss_BC15}')\n",
    "print(f'BC-15 - Test Accuracy: {test_accuracy_BC15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "155/155 [==============================] - 69s 445ms/step - loss: 0.3197 - accuracy: 0.8989 - val_loss: 0.0726 - val_accuracy: 0.9792\n",
      "Epoch 2/20\n",
      "155/155 [==============================] - 61s 397ms/step - loss: 0.0758 - accuracy: 0.9741 - val_loss: 0.2660 - val_accuracy: 0.8885\n",
      "Epoch 3/20\n",
      "155/155 [==============================] - 63s 405ms/step - loss: 0.0540 - accuracy: 0.9850 - val_loss: 1.6321 - val_accuracy: 0.5564\n",
      "Epoch 4/20\n",
      "155/155 [==============================] - 62s 403ms/step - loss: 0.0577 - accuracy: 0.9842 - val_loss: 0.0374 - val_accuracy: 0.9877\n",
      "Epoch 5/20\n",
      "155/155 [==============================] - 61s 394ms/step - loss: 0.0424 - accuracy: 0.9907 - val_loss: 0.0777 - val_accuracy: 0.9694\n",
      "Epoch 6/20\n",
      "155/155 [==============================] - 60s 386ms/step - loss: 0.0285 - accuracy: 0.9919 - val_loss: 1.0051 - val_accuracy: 0.7083\n",
      "Epoch 7/20\n",
      "155/155 [==============================] - 60s 384ms/step - loss: 0.0279 - accuracy: 0.9951 - val_loss: 0.5541 - val_accuracy: 0.8015\n",
      "Epoch 8/20\n",
      "155/155 [==============================] - 61s 392ms/step - loss: 0.0421 - accuracy: 0.9867 - val_loss: 0.1703 - val_accuracy: 0.9301\n",
      "Epoch 9/20\n",
      "155/155 [==============================] - 59s 383ms/step - loss: 0.0221 - accuracy: 0.9943 - val_loss: 0.0931 - val_accuracy: 0.9669\n",
      "Training Time: 557.7195162773132 seconds\n",
      "52/52 [==============================] - 9s 177ms/step - loss: 0.0423 - accuracy: 0.9808\n",
      "Huongthom - Test Loss: 0.04228588193655014\n",
      "Huongthom - Test Accuracy: 0.9807692170143127\n"
     ]
    }
   ],
   "source": [
    "#Huongthom\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Huongthom,\n",
    "    steps_per_epoch=train_generator_Huongthom.samples // train_generator_Huongthom.batch_size,\n",
    "    validation_data=validation_generator_Huongthom,\n",
    "    validation_steps=validation_generator_Huongthom.samples // validation_generator_Huongthom.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Huongthom, test_accuracy_Huongthom = model.evaluate(test_generator_Huongthom)\n",
    "print(f'Huongthom - Test Loss: {test_loss_Huongthom}')\n",
    "print(f'Huongthom - Test Accuracy: {test_accuracy_Huongthom}')\n",
    "\n",
    "#33p20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "107/107 [==============================] - 48s 442ms/step - loss: 0.2407 - accuracy: 0.9185 - val_loss: 0.1495 - val_accuracy: 0.9446\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 42s 388ms/step - loss: 0.0766 - accuracy: 0.9766 - val_loss: 0.0613 - val_accuracy: 0.9750\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 41s 387ms/step - loss: 0.0586 - accuracy: 0.9812 - val_loss: 0.0507 - val_accuracy: 0.9875\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.0585 - accuracy: 0.9824 - val_loss: 0.0555 - val_accuracy: 0.9839\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 41s 385ms/step - loss: 0.0574 - accuracy: 0.9859 - val_loss: 0.0445 - val_accuracy: 0.9893\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 41s 386ms/step - loss: 0.0319 - accuracy: 0.9930 - val_loss: 0.0744 - val_accuracy: 0.9696\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.0187 - accuracy: 0.9988 - val_loss: 0.0306 - val_accuracy: 0.9875\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 43s 404ms/step - loss: 0.0145 - accuracy: 0.9971 - val_loss: 0.0409 - val_accuracy: 0.9857\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 43s 399ms/step - loss: 0.0216 - accuracy: 0.9971 - val_loss: 0.1918 - val_accuracy: 0.9250\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 43s 400ms/step - loss: 0.0516 - accuracy: 0.9830 - val_loss: 0.0534 - val_accuracy: 0.9750\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 43s 406ms/step - loss: 0.0148 - accuracy: 0.9971 - val_loss: 0.0249 - val_accuracy: 0.9911\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 44s 414ms/step - loss: 0.0174 - accuracy: 0.9971 - val_loss: 0.0408 - val_accuracy: 0.9857\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 44s 411ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9911\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 44s 412ms/step - loss: 0.0180 - accuracy: 0.9965 - val_loss: 0.0533 - val_accuracy: 0.9786\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 44s 407ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9768\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 45s 416ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 0.0723 - val_accuracy: 0.9804\n",
      "Training Time: 691.3594818115234 seconds\n",
      "37/37 [==============================] - 10s 257ms/step - loss: 0.0209 - accuracy: 0.9896\n",
      "Nep87 - Test Loss: 0.020936185494065285\n",
      "Nep87 - Test Accuracy: 0.9896013736724854\n"
     ]
    }
   ],
   "source": [
    "# Nep87\n",
    "start = time.time()\n",
    "\n",
    "history_Nep87 = model.fit(\n",
    "    train_generator_Nep87,\n",
    "    steps_per_epoch=train_generator_Nep87.samples // train_generator_Nep87.batch_size,\n",
    "    validation_data=validation_generator_Nep87,\n",
    "    validation_steps=validation_generator_Nep87.samples // validation_generator_Nep87.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "\n",
    "test_loss_Nep87, test_accuracy_Nep87 = model.evaluate(test_generator_Nep87)\n",
    "print(f'Nep87 - Test Loss: {test_loss_Nep87}')\n",
    "print(f'Nep87 - Test Accuracy: {test_accuracy_Nep87}')\n",
    "\n",
    "#23p18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "112/112 [==============================] - 52s 461ms/step - loss: 0.4193 - accuracy: 0.8618 - val_loss: 2.1934 - val_accuracy: 0.5845\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 52s 462ms/step - loss: 0.1666 - accuracy: 0.9396 - val_loss: 0.1924 - val_accuracy: 0.9459\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 55s 490ms/step - loss: 0.0966 - accuracy: 0.9625 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 52s 466ms/step - loss: 0.0939 - accuracy: 0.9681 - val_loss: 0.4261 - val_accuracy: 0.8497\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 49s 436ms/step - loss: 0.0630 - accuracy: 0.9815 - val_loss: 0.1577 - val_accuracy: 0.9443\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 48s 425ms/step - loss: 0.0489 - accuracy: 0.9838 - val_loss: 0.0582 - val_accuracy: 0.9814\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 51s 457ms/step - loss: 0.0476 - accuracy: 0.9849 - val_loss: 0.0677 - val_accuracy: 0.9747\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 48s 427ms/step - loss: 0.0567 - accuracy: 0.9832 - val_loss: 0.1007 - val_accuracy: 0.9628\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 46s 414ms/step - loss: 0.0628 - accuracy: 0.9810 - val_loss: 0.1627 - val_accuracy: 0.9426\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 44s 395ms/step - loss: 0.0510 - accuracy: 0.9849 - val_loss: 0.1342 - val_accuracy: 0.9476\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 43s 387ms/step - loss: 0.0447 - accuracy: 0.9866 - val_loss: 0.0668 - val_accuracy: 0.9764\n",
      "Training Time: 541.3279159069061 seconds\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 0.0648 - accuracy: 0.9768\n",
      "Q5 - Test Loss: 0.06480606645345688\n",
      "Q5 - Test Accuracy: 0.9767827391624451\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "start = time.time()\n",
    "\n",
    "history_Q5 = model.fit(\n",
    "    train_generator_Q5,\n",
    "    steps_per_epoch=train_generator_Q5.samples // train_generator_Q5.batch_size,\n",
    "    validation_data=validation_generator_Q5,\n",
    "    validation_steps=validation_generator_Q5.samples // validation_generator_Q5.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "\n",
    "test_loss_Q5, test_accuracy_Q5 = model.evaluate(test_generator_Q5)\n",
    "print(f'Q5 - Test Loss: {test_loss_Q5}')\n",
    "print(f'Q5 - Test Accuracy: {test_accuracy_Q5}')\n",
    "\n",
    "#24p16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - 34s 447ms/step - loss: 0.4430 - accuracy: 0.8905 - val_loss: 1.2089 - val_accuracy: 0.7300\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 33s 445ms/step - loss: 0.0809 - accuracy: 0.9722 - val_loss: 0.0371 - val_accuracy: 0.9900\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 31s 416ms/step - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.3260 - val_accuracy: 0.8650\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 31s 415ms/step - loss: 0.0385 - accuracy: 0.9857 - val_loss: 0.0173 - val_accuracy: 0.9950\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 32s 430ms/step - loss: 0.1021 - accuracy: 0.9663 - val_loss: 0.0502 - val_accuracy: 0.9775\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 31s 415ms/step - loss: 0.0783 - accuracy: 0.9789 - val_loss: 0.0309 - val_accuracy: 0.9900\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 31s 420ms/step - loss: 0.0630 - accuracy: 0.9832 - val_loss: 0.0296 - val_accuracy: 0.9925\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 33s 433ms/step - loss: 0.0324 - accuracy: 0.9899 - val_loss: 0.0213 - val_accuracy: 0.9925\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 32s 433ms/step - loss: 0.0428 - accuracy: 0.9916 - val_loss: 0.0328 - val_accuracy: 0.9850\n",
      "Training Time: 290.1860909461975 seconds\n",
      "26/26 [==============================] - 5s 204ms/step - loss: 0.0388 - accuracy: 0.9900\n",
      "Thien_uu - Test Loss: 0.038809046149253845\n",
      "Thien_uu - Test Accuracy: 0.9900497794151306\n"
     ]
    }
   ],
   "source": [
    "# Thien_uu\n",
    "start = time.time()\n",
    "history_Thien_uu = model.fit(\n",
    "    train_generator_Thien_uu,\n",
    "    steps_per_epoch=train_generator_Thien_uu.samples // train_generator_Thien_uu.batch_size,\n",
    "    validation_data=validation_generator_Thien_uu,\n",
    "    validation_steps=validation_generator_Thien_uu.samples // validation_generator_Thien_uu.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Thien_uu, test_accuracy_Thien_uu = model.evaluate(test_generator_Thien_uu)\n",
    "print(f'Thien_uu - Test Loss: {test_loss_Thien_uu}')\n",
    "print(f'Thien_uu - Test Accuracy: {test_accuracy_Thien_uu}')\n",
    "#17p8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "155/155 [==============================] - 75s 486ms/step - loss: 0.5849 - accuracy: 0.8368 - val_loss: 0.3461 - val_accuracy: 0.8566\n",
      "Epoch 2/20\n",
      "155/155 [==============================] - 67s 433ms/step - loss: 0.1655 - accuracy: 0.9445 - val_loss: 0.2563 - val_accuracy: 0.8738\n",
      "Epoch 3/20\n",
      "155/155 [==============================] - 71s 455ms/step - loss: 0.0939 - accuracy: 0.9672 - val_loss: 0.0783 - val_accuracy: 0.9706\n",
      "Epoch 4/20\n",
      "155/155 [==============================] - 71s 459ms/step - loss: 0.0631 - accuracy: 0.9810 - val_loss: 0.0907 - val_accuracy: 0.9718\n",
      "Epoch 5/20\n",
      "155/155 [==============================] - 68s 434ms/step - loss: 0.0724 - accuracy: 0.9753 - val_loss: 0.0631 - val_accuracy: 0.9804\n",
      "Epoch 6/20\n",
      "155/155 [==============================] - 64s 411ms/step - loss: 0.0442 - accuracy: 0.9907 - val_loss: 0.1068 - val_accuracy: 0.9596\n",
      "Epoch 7/20\n",
      "155/155 [==============================] - 63s 407ms/step - loss: 0.0605 - accuracy: 0.9797 - val_loss: 0.1661 - val_accuracy: 0.9350\n",
      "Epoch 8/20\n",
      "155/155 [==============================] - 63s 404ms/step - loss: 0.0574 - accuracy: 0.9830 - val_loss: 0.0633 - val_accuracy: 0.9804\n",
      "Epoch 9/20\n",
      "155/155 [==============================] - 63s 403ms/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 0.0961 - val_accuracy: 0.9608\n",
      "Epoch 10/20\n",
      "155/155 [==============================] - 63s 405ms/step - loss: 0.0454 - accuracy: 0.9899 - val_loss: 0.0846 - val_accuracy: 0.9755\n",
      "Training Time: 678.2242105007172 seconds\n",
      "52/52 [==============================] - 10s 193ms/step - loss: 0.0545 - accuracy: 0.9819\n",
      "Xi23 - Test Loss: 0.05453778803348541\n",
      "Xi23 - Test Accuracy: 0.9819276928901672\n"
     ]
    }
   ],
   "source": [
    "# Xi23\n",
    "start = time.time()\n",
    "history_Xi23 = model.fit(\n",
    "    train_generator_Xi23,\n",
    "    steps_per_epoch=train_generator_Xi23.samples // train_generator_Xi23.batch_size,\n",
    "    validation_data=validation_generator_Xi23,\n",
    "    validation_steps=validation_generator_Xi23.samples // validation_generator_Xi23.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Xi23, test_accuracy_Xi23 = model.evaluate(test_generator_Xi23)\n",
    "print(f'Xi23 - Test Loss: {test_loss_Xi23}')\n",
    "print(f'Xi23 - Test Accuracy: {test_accuracy_Xi23}')\n",
    "#37p30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './models_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save(os.path.join(save_dir, 'model_BC15.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join(save_dir, 'model_Huongthom.h5'))\n",
    "model.save(os.path.join(save_dir, 'model_Nep87.h5'))\n",
    "model.save(os.path.join(save_dir, 'model_Q5.h5'))\n",
    "model.save(os.path.join(save_dir, 'model_Thien_uu.h5'))\n",
    "model.save(os.path.join(save_dir, 'model_Xi23.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              33555456  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1024)             4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,801,601\n",
      "Trainable params: 48,798,529\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "\n",
    "local_weights_file = './models_save/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = VGG16(input_shape=(256, 256, 3),  \n",
    "                          include_top=False,  \n",
    "                          weights='imagenet')  \n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('block5_pool')  # Layer cuối cùng của VGG16\n",
    "last_output = last_layer.output\n",
    "\n",
    "X = layers.Flatten()(last_output)\n",
    "X = layers.Dense(1024, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(512, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(1, activation='sigmoid')(X) \n",
    "\n",
    "model = Model(pre_trained_model.input, X)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 5s/step - accuracy: 0.9008 - loss: 0.2834 - val_accuracy: 0.8383 - val_loss: 0.4819\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/68\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:21\u001b[0m 5s/step - accuracy: 0.9688 - loss: 0.0982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\5530\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0982 - val_accuracy: 1.0000 - val_loss: 0.0670\n",
      "Epoch 3/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 5s/step - accuracy: 0.9770 - loss: 0.0662 - val_accuracy: 0.9307 - val_loss: 0.2384\n",
      "Epoch 4/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0634 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 5/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 5s/step - accuracy: 0.9833 - loss: 0.0532 - val_accuracy: 0.9701 - val_loss: 0.0762\n",
      "Epoch 6/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 3.8330e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 5s/step - accuracy: 0.9811 - loss: 0.0397 - val_accuracy: 0.9402 - val_loss: 0.1979\n",
      "Epoch 8/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0295\n",
      "Epoch 9/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 5s/step - accuracy: 0.9882 - loss: 0.0452 - val_accuracy: 0.9810 - val_loss: 0.0599\n",
      "Epoch 10/10\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 7.8566e-06\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 997ms/step - accuracy: 0.9538 - loss: 0.1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC-15 - Test Loss: 0.07526222616434097\n",
      "BC-15 - Test Accuracy: 0.9714673757553101\n",
      "Epoch 1/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 5s/step - accuracy: 0.8501 - loss: 0.8281 - val_accuracy: 0.5775 - val_loss: 2.1058\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0202 - val_accuracy: 0.6333 - val_loss: 1.3452\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 5s/step - accuracy: 0.9887 - loss: 0.0363 - val_accuracy: 0.9787 - val_loss: 0.0634\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 5s/step - accuracy: 0.9923 - loss: 0.0212 - val_accuracy: 0.9900 - val_loss: 0.0286\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 5s/step - accuracy: 0.9960 - loss: 0.0135 - val_accuracy: 0.9887 - val_loss: 0.0435\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0301 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 5s/step - accuracy: 0.9989 - loss: 0.0096 - val_accuracy: 0.9700 - val_loss: 0.0992\n",
      "Epoch 10/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.9333 - val_loss: 0.4486\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 983ms/step - accuracy: 0.9912 - loss: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huongthom - Test Loss: 0.0932769626379013\n",
      "Huongthom - Test Accuracy: 0.9711538553237915\n",
      "Epoch 1/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 5s/step - accuracy: 0.8815 - loss: 0.4201 - val_accuracy: 0.9449 - val_loss: 0.1430\n",
      "Epoch 2/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0421 - val_accuracy: 0.9667 - val_loss: 0.1540\n",
      "Epoch 3/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 5s/step - accuracy: 0.9879 - loss: 0.0327 - val_accuracy: 0.9816 - val_loss: 0.0553\n",
      "Epoch 4/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 1.0000 - val_loss: 0.0415\n",
      "Epoch 5/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 5s/step - accuracy: 0.9835 - loss: 0.0430 - val_accuracy: 0.9871 - val_loss: 0.0390\n",
      "Epoch 6/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.9667 - val_loss: 0.1300\n",
      "Epoch 7/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 5s/step - accuracy: 0.9958 - loss: 0.0165 - val_accuracy: 0.9871 - val_loss: 0.0351\n",
      "Epoch 8/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
      "Epoch 9/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 5s/step - accuracy: 0.9978 - loss: 0.0099 - val_accuracy: 0.9890 - val_loss: 0.0339\n",
      "Epoch 10/10\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 9.8758e-04 - val_accuracy: 0.9667 - val_loss: 0.1124\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 951ms/step - accuracy: 0.9859 - loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nep87 - Test Loss: 0.03051535040140152\n",
      "Nep87 - Test Accuracy: 0.984402060508728\n",
      "Epoch 1/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 5s/step - accuracy: 0.8092 - loss: 1.0189 - val_accuracy: 0.7587 - val_loss: 0.9751\n",
      "Epoch 2/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9062 - loss: 0.2308 - val_accuracy: 0.6786 - val_loss: 1.1196\n",
      "Epoch 3/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 5s/step - accuracy: 0.9652 - loss: 0.1046 - val_accuracy: 0.8993 - val_loss: 0.2750\n",
      "Epoch 4/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9688 - loss: 0.0440 - val_accuracy: 0.8929 - val_loss: 0.3197\n",
      "Epoch 5/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 5s/step - accuracy: 0.9783 - loss: 0.0526 - val_accuracy: 0.9392 - val_loss: 0.1830\n",
      "Epoch 6/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.8929 - val_loss: 0.4448\n",
      "Epoch 7/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 5s/step - accuracy: 0.9881 - loss: 0.0274 - val_accuracy: 0.9444 - val_loss: 0.1675\n",
      "Epoch 8/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 0.9286 - val_loss: 0.0744\n",
      "Epoch 9/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 5s/step - accuracy: 0.9923 - loss: 0.0243 - val_accuracy: 0.9271 - val_loss: 0.2239\n",
      "Epoch 10/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.9643 - val_loss: 0.0619\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 985ms/step - accuracy: 0.8857 - loss: 0.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 - Test Loss: 0.22860710322856903\n",
      "Q5 - Test Accuracy: 0.9253731369972229\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 5s/step - accuracy: 0.7798 - loss: 1.1415 - val_accuracy: 0.5807 - val_loss: 3.8148\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: 0.2853 - val_accuracy: 0.7059 - val_loss: 1.8798\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 5s/step - accuracy: 0.9928 - loss: 0.0378 - val_accuracy: 0.9844 - val_loss: 0.0533\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 1.0000 - val_loss: 0.0144\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 5s/step - accuracy: 0.9861 - loss: 0.0385 - val_accuracy: 0.9818 - val_loss: 0.0524\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 5s/step - accuracy: 0.9936 - loss: 0.0220 - val_accuracy: 0.9922 - val_loss: 0.0218\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0233 - val_accuracy: 1.0000 - val_loss: 0.0083\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 5s/step - accuracy: 0.9934 - loss: 0.0166 - val_accuracy: 0.9948 - val_loss: 0.0180\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 0.0096\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 961ms/step - accuracy: 0.9843 - loss: 0.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thien_uu - Test Loss: 0.09557463228702545\n",
      "Thien_uu - Test Accuracy: 0.9776119589805603\n",
      "Epoch 1/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 5s/step - accuracy: 0.7845 - loss: 0.9994 - val_accuracy: 0.7237 - val_loss: 0.9850\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0543 - val_accuracy: 0.5667 - val_loss: 2.1035\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 5s/step - accuracy: 0.9768 - loss: 0.0798 - val_accuracy: 0.9475 - val_loss: 0.1464\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9667 - val_loss: 0.0852\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 5s/step - accuracy: 0.9930 - loss: 0.0268 - val_accuracy: 0.9375 - val_loss: 0.1791\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0278 - val_accuracy: 0.8667 - val_loss: 0.2399\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 5s/step - accuracy: 0.9896 - loss: 0.0292 - val_accuracy: 0.9737 - val_loss: 0.0896\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 5s/step - accuracy: 0.9955 - loss: 0.0140 - val_accuracy: 0.9725 - val_loss: 0.1029\n",
      "Epoch 10/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.9375 - loss: 0.1362 - val_accuracy: 1.0000 - val_loss: 0.0091\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9930 - loss: 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xi23 - Test Loss: 0.026301443576812744\n",
      "Xi23 - Test Accuracy: 0.9903614521026611\n"
     ]
    }
   ],
   "source": [
    "#BC15\n",
    "history = model.fit(\n",
    "    train_generator_BC15,\n",
    "    steps_per_epoch=train_generator_BC15.samples // train_generator_BC15.batch_size,\n",
    "    validation_data=validation_generator_BC15,\n",
    "    validation_steps=validation_generator_BC15.samples // validation_generator_BC15.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_BC15, test_accuracy_BC15 = model.evaluate(test_generator_BC15)\n",
    "print(f'BC-15 - Test Loss: {test_loss_BC15}')\n",
    "print(f'BC-15 - Test Accuracy: {test_accuracy_BC15}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_BC15.h5'))\n",
    "\n",
    "#Huongthom\n",
    "history = model.fit(\n",
    "    train_generator_Huongthom,\n",
    "    steps_per_epoch=train_generator_Huongthom.samples // train_generator_Huongthom.batch_size,\n",
    "    validation_data=validation_generator_Huongthom,\n",
    "    validation_steps=validation_generator_Huongthom.samples // validation_generator_Huongthom.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Huongthom, test_accuracy_Huongthom = model.evaluate(test_generator_Huongthom)\n",
    "print(f'Huongthom - Test Loss: {test_loss_Huongthom}')\n",
    "print(f'Huongthom - Test Accuracy: {test_accuracy_Huongthom}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_Huongthom.h5'))\n",
    "\n",
    "# Nep87\n",
    "history = model.fit(\n",
    "    train_generator_Nep87,\n",
    "    steps_per_epoch=train_generator_Nep87.samples // train_generator_Nep87.batch_size,\n",
    "    validation_data=validation_generator_Nep87,\n",
    "    validation_steps=validation_generator_Nep87.samples // validation_generator_Nep87.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Nep87, test_accuracy_Nep87 = model.evaluate(test_generator_Nep87)\n",
    "print(f'Nep87 - Test Loss: {test_loss_Nep87}')\n",
    "print(f'Nep87 - Test Accuracy: {test_accuracy_Nep87}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_Nep87.h5'))\n",
    "\n",
    "# Q5\n",
    "history = model.fit(\n",
    "    train_generator_Q5,\n",
    "    steps_per_epoch=train_generator_Q5.samples // train_generator_Q5.batch_size,\n",
    "    validation_data=validation_generator_Q5,\n",
    "    validation_steps=validation_generator_Q5.samples // validation_generator_Q5.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Q5, test_accuracy_Q5 = model.evaluate(test_generator_Q5)\n",
    "print(f'Q5 - Test Loss: {test_loss_Q5}')\n",
    "print(f'Q5 - Test Accuracy: {test_accuracy_Q5}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_Q5.h5'))\n",
    "\n",
    "# Thien_uu\n",
    "history = model.fit(\n",
    "    train_generator_Thien_uu,\n",
    "    steps_per_epoch=train_generator_Thien_uu.samples // train_generator_Thien_uu.batch_size,\n",
    "    validation_data=validation_generator_Thien_uu,\n",
    "    validation_steps=validation_generator_Thien_uu.samples // validation_generator_Thien_uu.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Thien_uu, test_accuracy_Thien_uu = model.evaluate(test_generator_Thien_uu)\n",
    "print(f'Thien_uu - Test Loss: {test_loss_Thien_uu}')\n",
    "print(f'Thien_uu - Test Accuracy: {test_accuracy_Thien_uu}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_Thien_uu.h5'))\n",
    "\n",
    "# Xi23\n",
    "history = model.fit(\n",
    "    train_generator_Xi23,\n",
    "    steps_per_epoch=train_generator_Xi23.samples // train_generator_Xi23.batch_size,\n",
    "    validation_data=validation_generator_Xi23,\n",
    "    validation_steps=validation_generator_Xi23.samples // validation_generator_Xi23.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Xi23, test_accuracy_Xi23 = model.evaluate(test_generator_Xi23)\n",
    "print(f'Xi23 - Test Loss: {test_loss_Xi23}')\n",
    "print(f'Xi23 - Test Accuracy: {test_accuracy_Xi23}')\n",
    "model.save(os.path.join(save_dir, 'model_vgg_Xi23.h5'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, 262, 262, 3)  0          ['input_2[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 128, 128, 64  9408        ['zero_padding2d_2[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 128, 128, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, 130, 130, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 64, 64, 64)   0           ['zero_padding2d_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 64, 64, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 64, 64, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 64, 64, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 64, 64, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 64, 64, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 64, 64, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 64, 64, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 64, 64, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 64, 64, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 64, 64, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 64, 64, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 64, 64, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 64, 64, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 64, 64, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 64, 64, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 64, 64, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 64, 64, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 64, 64, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 64, 64, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 64, 64, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 32, 32, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 32, 32, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 32, 32, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 32, 32, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 32, 32, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 32, 32, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 32, 32, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 32, 32, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 32, 32, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 32, 32, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 32, 32, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 32, 32, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 32, 32, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 32, 32, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 32, 32, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 32, 32, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 32, 32, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 32, 32, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 32, 32, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 32, 32, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 32, 32, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 32, 32, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 32, 32, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 32, 32, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 32, 32, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 32, 32, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 32, 32, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 32, 32, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 32, 32, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 32, 32, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 32, 32, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 32, 32, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 32, 32, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 32, 32, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 32, 32, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 32, 32, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 32, 32, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 32, 32, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 32, 32, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 32, 32, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 32, 32, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 32, 32, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 32, 32, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 32, 32, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 32, 32, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 16, 16, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 16, 16, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 16, 16, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 16, 16, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 16, 16, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 16, 16, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 16, 16, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 16, 16, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 16, 16, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 16, 16, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 16, 16, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 16, 16, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 16, 16, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 16, 16, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 16, 16, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 16, 16, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 16, 16, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 16, 16, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 16, 16, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 16, 16, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 16, 16, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 16, 16, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 16, 16, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 16, 16, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 16, 16, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 16, 16, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 16, 16, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 16, 16, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 16, 16, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 16, 16, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 16, 16, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 16, 16, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 16, 16, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 16, 16, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 16, 16, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 16, 16, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 16, 16, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 16, 16, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 16, 16, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 16, 16, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 16, 16, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 16, 16, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 16, 16, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 16, 16, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 16, 16, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 16, 16, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 16, 16, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 16, 16, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 16, 16, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 16, 16, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 16, 16, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 16, 16, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 16, 16, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 16, 16, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 16, 16, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 16, 16, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 16, 16, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 16, 16, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 16, 16, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 16, 16, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 16, 16, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 16, 16, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 16, 16, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 16, 16, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 16, 16, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 16, 16, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 16, 16, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 16, 16, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 16, 16, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 16, 16, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 16, 16, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 16, 16, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 16, 16, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 16, 16, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 16, 16, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 16, 16, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 16, 16, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 16, 16, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 16, 16, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 16, 16, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 16, 16, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 16, 16, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 16, 16, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 16, 16, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 16, 16, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 16, 16, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 16, 16, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 16, 16, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 16, 16, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 16, 16, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 16, 16, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 8, 8, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 8, 8, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 8, 8, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 8, 8, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 8, 8, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 8, 8, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 8, 8, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 8, 8, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 8, 8, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 8, 8, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 8, 8, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 8, 8, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 8, 8, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 8, 8, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 8, 8, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 8, 8, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 8, 8, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 8, 8, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 8, 8, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 8, 8, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 8, 8, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 8, 8, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 8, 8, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 8, 8, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 8, 8, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 8, 8, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 8, 8, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 8, 8, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 8, 8, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 8, 8, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 8, 8, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 8, 8, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 8, 8, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 8, 8, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 8, 8, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 8, 8, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 8, 8, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 8, 8, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 8, 8, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 8, 8, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 8, 8, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 8, 8, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 8, 8, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 8, 8, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 8, 8, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 8, 8, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 8, 8, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 8, 8, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 8, 8, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 8, 8, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 8, 8, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 8, 8, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 8, 8, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 8, 8, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 8, 8, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 8, 8, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 8, 8, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 8, 8, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 8, 8, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 8, 8, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 8, 8, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 8, 8, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['relu[0][0]']                   \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          131200      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64)          256         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            65          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,177,793\n",
      "Trainable params: 7,093,761\n",
      "Non-trainable params: 84,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "local_weights_file = './models_save/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = DenseNet121(input_shape=(256, 256, 3),  \n",
    "                                include_top=False,  \n",
    "                                weights='imagenet')  \n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('relu')  # DenseNet last relu layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "X = layers.GlobalAveragePooling2D()(last_output)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(64, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(1, activation='sigmoid')(X)  # Binary classification\n",
    "\n",
    "model = Model(pre_trained_model.input, X)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 5, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC15\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_BC15,\n",
    "    steps_per_epoch=train_generator_BC15.samples // train_generator_BC15.batch_size,\n",
    "    validation_data=validation_generator_BC15,\n",
    "    validation_steps=validation_generator_BC15.samples // validation_generator_BC15.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_BC15, test_accuracy_BC15 = model.evaluate(test_generator_BC15)\n",
    "print(f'BC-15 - Test Loss: {test_loss_BC15}')\n",
    "print(f'BC-15 - Test Accuracy: {test_accuracy_BC15}')\n",
    "\n",
    "# Huongthom\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Huongthom,\n",
    "    steps_per_epoch=train_generator_Huongthom.samples // train_generator_Huongthom.batch_size,\n",
    "    validation_data=validation_generator_Huongthom,\n",
    "    validation_steps=validation_generator_Huongthom.samples // validation_generator_Huongthom.batch_size,\n",
    "    epochs=20, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Huongthom, test_accuracy_Huongthom = model.evaluate(test_generator_Huongthom)\n",
    "print(f'Huongthom - Test Loss: {test_loss_Huongthom}')\n",
    "print(f'Huongthom - Test Accuracy: {test_accuracy_Huongthom}')\n",
    "\n",
    "# Nep87\n",
    "# train_generator_Nep87.batch_size = 8\n",
    "# validation_generator_Nep87.batch_size = 8\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Nep87,\n",
    "    steps_per_epoch=train_generator_Nep87.samples // train_generator_Nep87.batch_size,\n",
    "    validation_data=validation_generator_Nep87,\n",
    "    validation_steps=validation_generator_Nep87.samples // validation_generator_Nep87.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Nep87, test_accuracy_Nep87 = model.evaluate(test_generator_Nep87)\n",
    "print(f'Nep87 - Test Loss: {test_loss_Nep87}')\n",
    "print(f'Nep87 - Test Accuracy: {test_accuracy_Nep87}')\n",
    "\n",
    "# Q5\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Q5,\n",
    "    steps_per_epoch=train_generator_Q5.samples // train_generator_Q5.batch_size,\n",
    "    validation_data=validation_generator_Q5,\n",
    "    validation_steps=validation_generator_Q5.samples // validation_generator_Q5.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Q5, test_accuracy_Q5 = model.evaluate(test_generator_Q5)\n",
    "print(f'Q5 - Test Loss: {test_loss_Q5}')\n",
    "print(f'Q5 - Test Accuracy: {test_accuracy_Q5}')\n",
    "\n",
    "# Thien_uu\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Thien_uu,\n",
    "    steps_per_epoch=train_generator_Thien_uu.samples // train_generator_Thien_uu.batch_size,\n",
    "    validation_data=validation_generator_Thien_uu,\n",
    "    validation_steps=validation_generator_Thien_uu.samples // validation_generator_Thien_uu.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Thien_uu, test_accuracy_Thien_uu = model.evaluate(test_generator_Thien_uu)\n",
    "print(f'Thien_uu - Test Loss: {test_loss_Thien_uu}')\n",
    "print(f'Thien_uu - Test Accuracy: {test_accuracy_Thien_uu}')\n",
    "\n",
    "# Xi23\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Xi23,\n",
    "    steps_per_epoch=train_generator_Xi23.samples // train_generator_Xi23.batch_size,\n",
    "    validation_data=validation_generator_Xi23,\n",
    "    validation_steps=validation_generator_Xi23.samples // validation_generator_Xi23.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Xi23, test_accuracy_Xi23 = model.evaluate(test_generator_Xi23)\n",
    "print(f'Xi23 - Test Loss: {test_loss_Xi23}')\n",
    "print(f'Xi23 - Test Accuracy: {test_accuracy_Xi23}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet50_weights_tf_dim_ordering_tf_kernels_notop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 8, 8, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 131072)       0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          16777344    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 128)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64)          256         ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64)           0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            65          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,374,145\n",
      "Trainable params: 40,320,641\n",
      "Non-trainable params: 53,504\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "275/275 [==============================] - 117s 393ms/step - loss: 0.5897 - accuracy: 0.7423 - val_loss: 1.0942 - val_accuracy: 0.5014\n",
      "Epoch 2/20\n",
      "275/275 [==============================] - 106s 387ms/step - loss: 0.3112 - accuracy: 0.8798 - val_loss: 4.4270 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "275/275 [==============================] - 106s 384ms/step - loss: 0.2382 - accuracy: 0.9126 - val_loss: 0.5529 - val_accuracy: 0.6970\n",
      "Epoch 4/20\n",
      "275/275 [==============================] - 106s 385ms/step - loss: 0.1886 - accuracy: 0.9312 - val_loss: 0.4937 - val_accuracy: 0.7283\n",
      "Epoch 5/20\n",
      "275/275 [==============================] - 106s 385ms/step - loss: 0.2435 - accuracy: 0.9153 - val_loss: 0.1429 - val_accuracy: 0.9416\n",
      "Epoch 6/20\n",
      "275/275 [==============================] - 107s 388ms/step - loss: 0.1936 - accuracy: 0.9344 - val_loss: 0.4529 - val_accuracy: 0.8193\n",
      "Epoch 7/20\n",
      "275/275 [==============================] - 105s 382ms/step - loss: 0.1574 - accuracy: 0.9408 - val_loss: 0.1353 - val_accuracy: 0.9457\n",
      "Epoch 8/20\n",
      "275/275 [==============================] - 107s 389ms/step - loss: 0.1547 - accuracy: 0.9490 - val_loss: 0.1823 - val_accuracy: 0.9402\n",
      "Epoch 9/20\n",
      "275/275 [==============================] - 105s 382ms/step - loss: 0.1403 - accuracy: 0.9581 - val_loss: 0.1736 - val_accuracy: 0.9293\n",
      "Epoch 10/20\n",
      "275/275 [==============================] - 107s 388ms/step - loss: 0.1308 - accuracy: 0.9572 - val_loss: 0.0824 - val_accuracy: 0.9715\n",
      "Epoch 11/20\n",
      "275/275 [==============================] - 107s 388ms/step - loss: 0.1306 - accuracy: 0.9617 - val_loss: 0.1064 - val_accuracy: 0.9592\n",
      "Epoch 12/20\n",
      "275/275 [==============================] - 106s 385ms/step - loss: 0.1474 - accuracy: 0.9495 - val_loss: 1.7159 - val_accuracy: 0.6644\n",
      "Epoch 13/20\n",
      "275/275 [==============================] - 105s 384ms/step - loss: 0.1364 - accuracy: 0.9545 - val_loss: 0.0700 - val_accuracy: 0.9796\n",
      "Epoch 14/20\n",
      "275/275 [==============================] - 105s 382ms/step - loss: 0.1014 - accuracy: 0.9654 - val_loss: 0.1392 - val_accuracy: 0.9592\n",
      "Epoch 15/20\n",
      "275/275 [==============================] - 106s 387ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.0752 - val_accuracy: 0.9674\n",
      "Epoch 16/20\n",
      "275/275 [==============================] - 106s 385ms/step - loss: 0.1484 - accuracy: 0.9513 - val_loss: 0.0746 - val_accuracy: 0.9728\n",
      "Epoch 17/20\n",
      "275/275 [==============================] - 105s 383ms/step - loss: 0.1111 - accuracy: 0.9645 - val_loss: 0.0945 - val_accuracy: 0.9660\n",
      "Epoch 18/20\n",
      "275/275 [==============================] - 106s 387ms/step - loss: 0.1793 - accuracy: 0.9408 - val_loss: 0.0802 - val_accuracy: 0.9755\n",
      "Training Time: 1918.9062752723694 seconds\n",
      "46/46 [==============================] - 9s 176ms/step - loss: 0.1348 - accuracy: 0.9579\n",
      "BC-15 - Test Loss: 0.1347937136888504\n",
      "BC-15 - Test Accuracy: 0.957880437374115\n",
      "Epoch 1/20\n",
      "155/155 [==============================] - 106s 668ms/step - loss: 0.3582 - accuracy: 0.8823 - val_loss: 0.9118 - val_accuracy: 0.6716\n",
      "Epoch 2/20\n",
      "155/155 [==============================] - 101s 649ms/step - loss: 0.1265 - accuracy: 0.9587 - val_loss: 0.1683 - val_accuracy: 0.9412\n",
      "Epoch 3/20\n",
      "155/155 [==============================] - 99s 638ms/step - loss: 0.0731 - accuracy: 0.9798 - val_loss: 0.1274 - val_accuracy: 0.9547\n",
      "Epoch 4/20\n",
      "155/155 [==============================] - 100s 647ms/step - loss: 0.0677 - accuracy: 0.9769 - val_loss: 0.2409 - val_accuracy: 0.9069\n",
      "Epoch 5/20\n",
      "155/155 [==============================] - 102s 661ms/step - loss: 0.0760 - accuracy: 0.9745 - val_loss: 0.0924 - val_accuracy: 0.9681\n",
      "Epoch 6/20\n",
      "155/155 [==============================] - 102s 660ms/step - loss: 0.0794 - accuracy: 0.9769 - val_loss: 0.0917 - val_accuracy: 0.9645\n",
      "Epoch 7/20\n",
      "155/155 [==============================] - 100s 642ms/step - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.1424 - val_accuracy: 0.9547\n",
      "Epoch 8/20\n",
      "155/155 [==============================] - 100s 648ms/step - loss: 0.0672 - accuracy: 0.9745 - val_loss: 0.1080 - val_accuracy: 0.9645\n",
      "Epoch 9/20\n",
      "155/155 [==============================] - 99s 641ms/step - loss: 0.0530 - accuracy: 0.9822 - val_loss: 0.0874 - val_accuracy: 0.9718\n",
      "Epoch 10/20\n",
      "155/155 [==============================] - 100s 645ms/step - loss: 0.0469 - accuracy: 0.9854 - val_loss: 0.4741 - val_accuracy: 0.7904\n",
      "Epoch 11/20\n",
      "155/155 [==============================] - 101s 649ms/step - loss: 0.0354 - accuracy: 0.9903 - val_loss: 0.3781 - val_accuracy: 0.8701\n",
      "Epoch 12/20\n",
      "155/155 [==============================] - 101s 650ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.3338 - val_accuracy: 0.8824\n",
      "Epoch 13/20\n",
      "155/155 [==============================] - 101s 650ms/step - loss: 0.0220 - accuracy: 0.9951 - val_loss: 0.0664 - val_accuracy: 0.9804\n",
      "Epoch 14/20\n",
      "155/155 [==============================] - 99s 641ms/step - loss: 0.0171 - accuracy: 0.9960 - val_loss: 0.1291 - val_accuracy: 0.9620\n",
      "Epoch 15/20\n",
      "155/155 [==============================] - 100s 646ms/step - loss: 0.0287 - accuracy: 0.9943 - val_loss: 1.5539 - val_accuracy: 0.6152\n",
      "Epoch 16/20\n",
      "155/155 [==============================] - 100s 646ms/step - loss: 0.0343 - accuracy: 0.9899 - val_loss: 0.0866 - val_accuracy: 0.9743\n",
      "Epoch 17/20\n",
      "155/155 [==============================] - 100s 643ms/step - loss: 0.0416 - accuracy: 0.9891 - val_loss: 0.1084 - val_accuracy: 0.9547\n",
      "Epoch 18/20\n",
      "155/155 [==============================] - 99s 638ms/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 0.0996 - val_accuracy: 0.9608\n",
      "Training Time: 1811.0696816444397 seconds\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 0.0579 - accuracy: 0.9856\n",
      "Huongthom - Test Loss: 0.05788598582148552\n",
      "Huongthom - Test Accuracy: 0.9855769276618958\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 74s 692ms/step - loss: 0.3048 - accuracy: 0.8933 - val_loss: 0.8298 - val_accuracy: 0.7161\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 69s 646ms/step - loss: 0.1249 - accuracy: 0.9531 - val_loss: 0.9572 - val_accuracy: 0.6036\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 68s 640ms/step - loss: 0.0779 - accuracy: 0.9736 - val_loss: 0.3267 - val_accuracy: 0.8536\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 69s 642ms/step - loss: 0.0994 - accuracy: 0.9642 - val_loss: 1.0169 - val_accuracy: 0.6143\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 69s 641ms/step - loss: 0.0865 - accuracy: 0.9736 - val_loss: 0.7046 - val_accuracy: 0.5804\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 69s 642ms/step - loss: 0.0538 - accuracy: 0.9807 - val_loss: 0.2143 - val_accuracy: 0.9036\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 70s 651ms/step - loss: 0.0395 - accuracy: 0.9906 - val_loss: 0.0508 - val_accuracy: 0.9839\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 69s 644ms/step - loss: 0.0721 - accuracy: 0.9760 - val_loss: 2.4956 - val_accuracy: 0.5161\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 69s 641ms/step - loss: 0.0489 - accuracy: 0.9830 - val_loss: 0.0854 - val_accuracy: 0.9732\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 69s 645ms/step - loss: 0.0498 - accuracy: 0.9853 - val_loss: 0.0644 - val_accuracy: 0.9839\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 69s 643ms/step - loss: 0.0467 - accuracy: 0.9824 - val_loss: 0.7808 - val_accuracy: 0.7304\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 69s 641ms/step - loss: 0.0339 - accuracy: 0.9859 - val_loss: 0.2620 - val_accuracy: 0.9107\n",
      "Training Time: 832.0465936660767 seconds\n",
      "37/37 [==============================] - 8s 216ms/step - loss: 0.0927 - accuracy: 0.9619\n",
      "Nep87 - Test Loss: 0.09271296113729477\n",
      "Nep87 - Test Accuracy: 0.9618717432022095\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 78s 695ms/step - loss: 0.5260 - accuracy: 0.8293 - val_loss: 0.4445 - val_accuracy: 0.8125\n",
      "Epoch 2/20\n",
      " 90/112 [=======================>......] - ETA: 12s - loss: 0.2962 - accuracy: 0.8753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Q5\u001b[39;00m\n\u001b[0;32m     92\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 93\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator_Q5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator_Q5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator_Q5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator_Q5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator_Q5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator_Q5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Load the pre-trained ResNet50 model directly with ImageNet weights\n",
    "pre_trained_model = ResNet50(input_shape=(256, 256, 3),  \n",
    "                              include_top=False,\n",
    "                              weights='imagenet')\n",
    "\n",
    "# Get the output of the last layer\n",
    "last_layer = pre_trained_model.get_layer('conv5_block3_out')  # Last convolutional layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# Build the custom classifier on top of the pre-trained model\n",
    "X = layers.Flatten()(last_output)\n",
    "X = layers.Dense(128, activation='relu')(X)  \n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X) \n",
    "X = layers.Dense(64, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "# Create the model\n",
    "model = Model(pre_trained_model.input, X)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# BC15\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_BC15,\n",
    "    steps_per_epoch=train_generator_BC15.samples // train_generator_BC15.batch_size,\n",
    "    validation_data=validation_generator_BC15,\n",
    "    validation_steps=validation_generator_BC15.samples // validation_generator_BC15.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_BC15, test_accuracy_BC15 = model.evaluate(test_generator_BC15)\n",
    "print(f'BC-15 - Test Loss: {test_loss_BC15}')\n",
    "print(f'BC-15 - Test Accuracy: {test_accuracy_BC15}')\n",
    "\n",
    "# Huongthom\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Huongthom,\n",
    "    steps_per_epoch=train_generator_Huongthom.samples // train_generator_Huongthom.batch_size,\n",
    "    validation_data=validation_generator_Huongthom,\n",
    "    validation_steps=validation_generator_Huongthom.samples // validation_generator_Huongthom.batch_size,\n",
    "    epochs=20, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Huongthom, test_accuracy_Huongthom = model.evaluate(test_generator_Huongthom)\n",
    "print(f'Huongthom - Test Loss: {test_loss_Huongthom}')\n",
    "print(f'Huongthom - Test Accuracy: {test_accuracy_Huongthom}')\n",
    "\n",
    "# Nep87\n",
    "# train_generator_Nep87.batch_size = 8\n",
    "# validation_generator_Nep87.batch_size = 8\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Nep87,\n",
    "    steps_per_epoch=train_generator_Nep87.samples // train_generator_Nep87.batch_size,\n",
    "    validation_data=validation_generator_Nep87,\n",
    "    validation_steps=validation_generator_Nep87.samples // validation_generator_Nep87.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Nep87, test_accuracy_Nep87 = model.evaluate(test_generator_Nep87)\n",
    "print(f'Nep87 - Test Loss: {test_loss_Nep87}')\n",
    "print(f'Nep87 - Test Accuracy: {test_accuracy_Nep87}')\n",
    "\n",
    "# Q5\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Q5,\n",
    "    steps_per_epoch=train_generator_Q5.samples // train_generator_Q5.batch_size,\n",
    "    validation_data=validation_generator_Q5,\n",
    "    validation_steps=validation_generator_Q5.samples // validation_generator_Q5.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Q5, test_accuracy_Q5 = model.evaluate(test_generator_Q5)\n",
    "print(f'Q5 - Test Loss: {test_loss_Q5}')\n",
    "print(f'Q5 - Test Accuracy: {test_accuracy_Q5}')\n",
    "\n",
    "# Thien_uu\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Thien_uu,\n",
    "    steps_per_epoch=train_generator_Thien_uu.samples // train_generator_Thien_uu.batch_size,\n",
    "    validation_data=validation_generator_Thien_uu,\n",
    "    validation_steps=validation_generator_Thien_uu.samples // validation_generator_Thien_uu.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Thien_uu, test_accuracy_Thien_uu = model.evaluate(test_generator_Thien_uu)\n",
    "print(f'Thien_uu - Test Loss: {test_loss_Thien_uu}')\n",
    "print(f'Thien_uu - Test Accuracy: {test_accuracy_Thien_uu}')\n",
    "\n",
    "# Xi23\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_generator_Xi23,\n",
    "    steps_per_epoch=train_generator_Xi23.samples // train_generator_Xi23.batch_size,\n",
    "    validation_data=validation_generator_Xi23,\n",
    "    validation_steps=validation_generator_Xi23.samples // validation_generator_Xi23.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training Time: {end - start} seconds\")\n",
    "test_loss_Xi23, test_accuracy_Xi23 = model.evaluate(test_generator_Xi23)\n",
    "print(f'Xi23 - Test Loss: {test_loss_Xi23}')\n",
    "print(f'Xi23 - Test Accuracy: {test_accuracy_Xi23}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " Conv1 (Conv2D)              (None, 128, 128, 32)      864       \n",
      "                                                                 \n",
      " bn_Conv1 (BatchNormalizatio  (None, 128, 128, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " Conv1_relu (ReLU)           (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 32)               0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              33792     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 566,241\n",
      "Trainable params: 563,105\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import os\n",
    "# pre_trained_model = EfficientNetB0(input_shape=(256, 256, 3),  \n",
    "#                                     include_top=False,  \n",
    "#                                     weights='imagenet')  \n",
    "\n",
    "\n",
    "# last_layer = pre_trained_model.get_layer('top_conv')  # EfficientNet last convolutional layer\n",
    "# last_output = last_layer.output\n",
    "\n",
    "# X = layers.GlobalAveragePooling2D()(last_output)\n",
    "# X = layers.Dense(1024, activation='relu')(X)\n",
    "# X = layers.BatchNormalization()(X)\n",
    "# X = layers.Dropout(0.5)(X)\n",
    "# X = layers.Dense(512, activation='relu')(X)\n",
    "# X = layers.BatchNormalization()(X)\n",
    "# X = layers.Dropout(0.5)(X)\n",
    "# X = layers.Dense(1, activation='sigmoid')(X) \n",
    "\n",
    "# model = Model(pre_trained_model.input, X)\n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "pre_trained_model = MobileNetV2(input_shape=(256, 256, 3),  \n",
    "                                 include_top=False,  \n",
    "                                 weights='imagenet')\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file, by_name=True, skip_mismatch=True)\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('Conv1_relu')  \n",
    "last_output = last_layer.output\n",
    "\n",
    "X = layers.GlobalAveragePooling2D()(last_output)\n",
    "X = layers.Dense(1024, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(512, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(1, activation='sigmoid')(X)  # Binary classification\n",
    "\n",
    "model = Model(pre_trained_model.input, X)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "275/275 [==============================] - 7s 21ms/step - loss: 0.8246 - accuracy: 0.6073 - val_loss: 0.8713 - val_accuracy: 0.5014\n",
      "Epoch 2/100\n",
      "275/275 [==============================] - 5s 18ms/step - loss: 0.7799 - accuracy: 0.6295 - val_loss: 2.1423 - val_accuracy: 0.5014\n",
      "Epoch 3/100\n",
      "275/275 [==============================] - 5s 19ms/step - loss: 0.7363 - accuracy: 0.6430 - val_loss: 1.2337 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "275/275 [==============================] - 5s 19ms/step - loss: 0.7178 - accuracy: 0.6566 - val_loss: 0.7332 - val_accuracy: 0.5149\n",
      "Epoch 5/100\n",
      "275/275 [==============================] - 5s 20ms/step - loss: 0.7238 - accuracy: 0.6526 - val_loss: 1.1057 - val_accuracy: 0.4986\n",
      "Epoch 6/100\n",
      "275/275 [==============================] - 6s 20ms/step - loss: 0.6783 - accuracy: 0.6767 - val_loss: 3.3303 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "275/275 [==============================] - 5s 20ms/step - loss: 0.6788 - accuracy: 0.6730 - val_loss: 4.3495 - val_accuracy: 0.4986\n",
      "Epoch 8/100\n",
      "275/275 [==============================] - 6s 21ms/step - loss: 0.6760 - accuracy: 0.6753 - val_loss: 5.8596 - val_accuracy: 0.4986\n",
      "Epoch 9/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.6787 - accuracy: 0.6639 - val_loss: 5.7430 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.6601 - accuracy: 0.6785 - val_loss: 3.0567 - val_accuracy: 0.4986\n",
      "Epoch 11/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.6316 - accuracy: 0.6812 - val_loss: 1.7674 - val_accuracy: 0.5014\n",
      "Epoch 12/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.6639 - accuracy: 0.6758 - val_loss: 4.3438 - val_accuracy: 0.4986\n",
      "Epoch 13/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.6671 - accuracy: 0.6858 - val_loss: 1.7592 - val_accuracy: 0.5530\n",
      "Epoch 14/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.6072 - accuracy: 0.7117 - val_loss: 1.3994 - val_accuracy: 0.4511\n",
      "Epoch 15/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.6222 - accuracy: 0.7017 - val_loss: 1.0222 - val_accuracy: 0.5611\n",
      "Epoch 16/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.6434 - accuracy: 0.6862 - val_loss: 2.1912 - val_accuracy: 0.5149\n",
      "Epoch 17/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.6099 - accuracy: 0.6954 - val_loss: 0.8313 - val_accuracy: 0.5842\n",
      "Epoch 18/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.6044 - accuracy: 0.7004 - val_loss: 1.0879 - val_accuracy: 0.5802\n",
      "Epoch 19/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.6033 - accuracy: 0.6995 - val_loss: 1.0249 - val_accuracy: 0.5394\n",
      "Epoch 20/100\n",
      "275/275 [==============================] - 11s 39ms/step - loss: 0.6024 - accuracy: 0.6913 - val_loss: 2.7223 - val_accuracy: 0.5027\n",
      "Epoch 21/100\n",
      "275/275 [==============================] - 9s 34ms/step - loss: 0.5842 - accuracy: 0.7040 - val_loss: 1.0223 - val_accuracy: 0.5095\n",
      "Epoch 22/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.6068 - accuracy: 0.7013 - val_loss: 1.5093 - val_accuracy: 0.5598\n",
      "Epoch 23/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5832 - accuracy: 0.7186 - val_loss: 1.3893 - val_accuracy: 0.5571\n",
      "Epoch 24/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.5774 - accuracy: 0.7227 - val_loss: 1.1491 - val_accuracy: 0.6019\n",
      "Epoch 25/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.5888 - accuracy: 0.7095 - val_loss: 0.8207 - val_accuracy: 0.6005\n",
      "Epoch 26/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5536 - accuracy: 0.7236 - val_loss: 1.0795 - val_accuracy: 0.5394\n",
      "Epoch 27/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5631 - accuracy: 0.7163 - val_loss: 2.4246 - val_accuracy: 0.5082\n",
      "Epoch 28/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5706 - accuracy: 0.7163 - val_loss: 0.9342 - val_accuracy: 0.6236\n",
      "Epoch 29/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5622 - accuracy: 0.7277 - val_loss: 3.0985 - val_accuracy: 0.5122\n",
      "Epoch 30/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5588 - accuracy: 0.7254 - val_loss: 1.6568 - val_accuracy: 0.5761\n",
      "Epoch 31/100\n",
      "275/275 [==============================] - 6s 24ms/step - loss: 0.5391 - accuracy: 0.7240 - val_loss: 1.0590 - val_accuracy: 0.5503\n",
      "Epoch 32/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5672 - accuracy: 0.7218 - val_loss: 1.4591 - val_accuracy: 0.5802\n",
      "Epoch 33/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5301 - accuracy: 0.7309 - val_loss: 0.7985 - val_accuracy: 0.6209\n",
      "Epoch 34/100\n",
      "275/275 [==============================] - 9s 31ms/step - loss: 0.5503 - accuracy: 0.7236 - val_loss: 2.6991 - val_accuracy: 0.5122\n",
      "Epoch 35/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.5391 - accuracy: 0.7318 - val_loss: 1.0634 - val_accuracy: 0.5924\n",
      "Epoch 36/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5351 - accuracy: 0.7291 - val_loss: 1.1196 - val_accuracy: 0.5938\n",
      "Epoch 37/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5459 - accuracy: 0.7263 - val_loss: 0.8002 - val_accuracy: 0.6033\n",
      "Epoch 38/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.5501 - accuracy: 0.7136 - val_loss: 2.0353 - val_accuracy: 0.5326\n",
      "Epoch 39/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5306 - accuracy: 0.7345 - val_loss: 0.8188 - val_accuracy: 0.6168\n",
      "Epoch 40/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5370 - accuracy: 0.7286 - val_loss: 3.1148 - val_accuracy: 0.5340\n",
      "Epoch 41/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5333 - accuracy: 0.7350 - val_loss: 1.1842 - val_accuracy: 0.6046\n",
      "Epoch 42/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.5338 - accuracy: 0.7318 - val_loss: 0.7637 - val_accuracy: 0.5992\n",
      "Epoch 43/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5330 - accuracy: 0.7313 - val_loss: 1.0579 - val_accuracy: 0.5571\n",
      "Epoch 44/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.5324 - accuracy: 0.7277 - val_loss: 2.0802 - val_accuracy: 0.5258\n",
      "Epoch 45/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5341 - accuracy: 0.7254 - val_loss: 0.8013 - val_accuracy: 0.5761\n",
      "Epoch 46/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5175 - accuracy: 0.7427 - val_loss: 0.8933 - val_accuracy: 0.5870\n",
      "Epoch 47/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5195 - accuracy: 0.7400 - val_loss: 0.6866 - val_accuracy: 0.6440\n",
      "Epoch 48/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5083 - accuracy: 0.7445 - val_loss: 0.7702 - val_accuracy: 0.6916\n",
      "Epoch 49/100\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.5175 - accuracy: 0.7400 - val_loss: 0.5816 - val_accuracy: 0.7174\n",
      "Epoch 50/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.5354 - accuracy: 0.7400 - val_loss: 1.7221 - val_accuracy: 0.5149\n",
      "Epoch 51/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5266 - accuracy: 0.7418 - val_loss: 0.9830 - val_accuracy: 0.6264\n",
      "Epoch 52/100\n",
      "275/275 [==============================] - 6s 23ms/step - loss: 0.5078 - accuracy: 0.7350 - val_loss: 3.8730 - val_accuracy: 0.4986\n",
      "Epoch 53/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.5113 - accuracy: 0.7486 - val_loss: 2.3841 - val_accuracy: 0.5326\n",
      "Epoch 54/100\n",
      "275/275 [==============================] - 6s 22ms/step - loss: 0.5087 - accuracy: 0.7441 - val_loss: 0.7557 - val_accuracy: 0.6291\n",
      "Epoch 55/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.4991 - accuracy: 0.7482 - val_loss: 0.7437 - val_accuracy: 0.6277\n",
      "Epoch 56/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.5141 - accuracy: 0.7427 - val_loss: 0.5608 - val_accuracy: 0.7120\n",
      "Epoch 57/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.5000 - accuracy: 0.7454 - val_loss: 0.9053 - val_accuracy: 0.6264\n",
      "Epoch 58/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5084 - accuracy: 0.7518 - val_loss: 0.5117 - val_accuracy: 0.7323\n",
      "Epoch 59/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.4965 - accuracy: 0.7495 - val_loss: 1.0372 - val_accuracy: 0.5584\n",
      "Epoch 60/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4966 - accuracy: 0.7564 - val_loss: 0.5946 - val_accuracy: 0.6739\n",
      "Epoch 61/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.5171 - accuracy: 0.7441 - val_loss: 1.4446 - val_accuracy: 0.5734\n",
      "Epoch 62/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5042 - accuracy: 0.7500 - val_loss: 0.7299 - val_accuracy: 0.6427\n",
      "Epoch 63/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5073 - accuracy: 0.7500 - val_loss: 0.9727 - val_accuracy: 0.5978\n",
      "Epoch 64/100\n",
      "275/275 [==============================] - 7s 24ms/step - loss: 0.4945 - accuracy: 0.7568 - val_loss: 1.3944 - val_accuracy: 0.5571\n",
      "Epoch 65/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4954 - accuracy: 0.7596 - val_loss: 3.2804 - val_accuracy: 0.5149\n",
      "Epoch 66/100\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.4980 - accuracy: 0.7577 - val_loss: 2.8723 - val_accuracy: 0.5258\n",
      "Epoch 67/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5158 - accuracy: 0.7413 - val_loss: 0.6093 - val_accuracy: 0.6508\n",
      "Epoch 68/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4912 - accuracy: 0.7600 - val_loss: 0.5574 - val_accuracy: 0.6997\n",
      "Epoch 69/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4919 - accuracy: 0.7587 - val_loss: 1.9132 - val_accuracy: 0.5625\n",
      "Epoch 70/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4900 - accuracy: 0.7582 - val_loss: 0.5240 - val_accuracy: 0.7378\n",
      "Epoch 71/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.5008 - accuracy: 0.7464 - val_loss: 1.8940 - val_accuracy: 0.5340\n",
      "Epoch 72/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.4772 - accuracy: 0.7637 - val_loss: 2.0564 - val_accuracy: 0.5217\n",
      "Epoch 73/100\n",
      "275/275 [==============================] - 8s 30ms/step - loss: 0.4848 - accuracy: 0.7659 - val_loss: 1.0618 - val_accuracy: 0.5693\n",
      "Epoch 74/100\n",
      "275/275 [==============================] - 8s 30ms/step - loss: 0.4969 - accuracy: 0.7536 - val_loss: 1.0754 - val_accuracy: 0.5747\n",
      "Epoch 75/100\n",
      "275/275 [==============================] - 9s 33ms/step - loss: 0.4914 - accuracy: 0.7546 - val_loss: 2.0061 - val_accuracy: 0.5054\n",
      "Epoch 76/100\n",
      "275/275 [==============================] - 9s 31ms/step - loss: 0.4907 - accuracy: 0.7614 - val_loss: 2.5051 - val_accuracy: 0.5258\n",
      "Epoch 77/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4804 - accuracy: 0.7605 - val_loss: 2.4464 - val_accuracy: 0.5476\n",
      "Epoch 78/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.4689 - accuracy: 0.7691 - val_loss: 0.8590 - val_accuracy: 0.6277\n",
      "Epoch 79/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.4936 - accuracy: 0.7568 - val_loss: 0.5395 - val_accuracy: 0.7405\n",
      "Epoch 80/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.5053 - accuracy: 0.7423 - val_loss: 2.6129 - val_accuracy: 0.4986\n",
      "Epoch 81/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4859 - accuracy: 0.7495 - val_loss: 0.5775 - val_accuracy: 0.7024\n",
      "Epoch 82/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4930 - accuracy: 0.7641 - val_loss: 0.6152 - val_accuracy: 0.6236\n",
      "Epoch 83/100\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.4918 - accuracy: 0.7500 - val_loss: 0.8681 - val_accuracy: 0.5842\n",
      "Epoch 84/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4822 - accuracy: 0.7573 - val_loss: 2.8722 - val_accuracy: 0.5258\n",
      "Epoch 85/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.4891 - accuracy: 0.7582 - val_loss: 0.8326 - val_accuracy: 0.6603\n",
      "Epoch 86/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.5065 - accuracy: 0.7445 - val_loss: 2.3868 - val_accuracy: 0.5136\n",
      "Epoch 87/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4845 - accuracy: 0.7637 - val_loss: 1.4326 - val_accuracy: 0.5435\n",
      "Epoch 88/100\n",
      "275/275 [==============================] - 7s 25ms/step - loss: 0.4898 - accuracy: 0.7559 - val_loss: 0.5607 - val_accuracy: 0.7201\n",
      "Epoch 89/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4962 - accuracy: 0.7500 - val_loss: 0.6547 - val_accuracy: 0.6997\n",
      "Epoch 90/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.4961 - accuracy: 0.7500 - val_loss: 2.0817 - val_accuracy: 0.5707\n",
      "Epoch 91/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4796 - accuracy: 0.7591 - val_loss: 2.2737 - val_accuracy: 0.5611\n",
      "Epoch 92/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4734 - accuracy: 0.7509 - val_loss: 1.7390 - val_accuracy: 0.5163\n",
      "Epoch 93/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4828 - accuracy: 0.7600 - val_loss: 2.6079 - val_accuracy: 0.5027\n",
      "Epoch 94/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4812 - accuracy: 0.7609 - val_loss: 0.8828 - val_accuracy: 0.6277\n",
      "Epoch 95/100\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.4912 - accuracy: 0.7546 - val_loss: 1.2542 - val_accuracy: 0.5734\n",
      "Epoch 96/100\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.4741 - accuracy: 0.7655 - val_loss: 1.0884 - val_accuracy: 0.6277\n",
      "Epoch 97/100\n",
      "275/275 [==============================] - 8s 29ms/step - loss: 0.4797 - accuracy: 0.7546 - val_loss: 1.3486 - val_accuracy: 0.5978\n",
      "Epoch 98/100\n",
      "275/275 [==============================] - 8s 29ms/step - loss: 0.4926 - accuracy: 0.7582 - val_loss: 4.3524 - val_accuracy: 0.5068\n",
      "Epoch 99/100\n",
      "275/275 [==============================] - 9s 32ms/step - loss: 0.4745 - accuracy: 0.7632 - val_loss: 1.1025 - val_accuracy: 0.6141\n",
      "Epoch 100/100\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.4749 - accuracy: 0.7723 - val_loss: 4.3653 - val_accuracy: 0.5095\n",
      "23/23 [==============================] - 2s 77ms/step - loss: 4.3944 - accuracy: 0.5136\n",
      "BC-15 - Test Loss: 4.394431114196777\n",
      "BC-15 - Test Accuracy: 0.5135869383811951\n",
      "Epoch 1/10\n",
      "311/311 [==============================] - 8s 27ms/step - loss: 0.4780 - accuracy: 0.7834 - val_loss: 12.3229 - val_accuracy: 0.4939\n",
      "Epoch 2/10\n",
      "311/311 [==============================] - 8s 27ms/step - loss: 0.3470 - accuracy: 0.8384 - val_loss: 6.5601 - val_accuracy: 0.4915\n",
      "Epoch 3/10\n",
      "311/311 [==============================] - 8s 27ms/step - loss: 0.3224 - accuracy: 0.8597 - val_loss: 1.4527 - val_accuracy: 0.6044\n",
      "Epoch 4/10\n",
      "311/311 [==============================] - 8s 26ms/step - loss: 0.2995 - accuracy: 0.8698 - val_loss: 1.5543 - val_accuracy: 0.6129\n",
      "Epoch 5/10\n",
      "311/311 [==============================] - 8s 26ms/step - loss: 0.2737 - accuracy: 0.8899 - val_loss: 1.8036 - val_accuracy: 0.6117\n",
      "Epoch 6/10\n",
      "311/311 [==============================] - 8s 27ms/step - loss: 0.2691 - accuracy: 0.8855 - val_loss: 2.8136 - val_accuracy: 0.5534\n",
      "Epoch 7/10\n",
      "311/311 [==============================] - 9s 28ms/step - loss: 0.2639 - accuracy: 0.8919 - val_loss: 0.8166 - val_accuracy: 0.6845\n",
      "Epoch 8/10\n",
      "311/311 [==============================] - 8s 26ms/step - loss: 0.2403 - accuracy: 0.9064 - val_loss: 2.9848 - val_accuracy: 0.5206\n",
      "Epoch 9/10\n",
      "311/311 [==============================] - 8s 26ms/step - loss: 0.2043 - accuracy: 0.9216 - val_loss: 0.4336 - val_accuracy: 0.8046\n",
      "Epoch 10/10\n",
      "311/311 [==============================] - 9s 29ms/step - loss: 0.2048 - accuracy: 0.9144 - val_loss: 1.6973 - val_accuracy: 0.5765\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.6204 - accuracy: 0.5865\n",
      "Huongthom - Test Loss: 1.6203726530075073\n",
      "Huongthom - Test Accuracy: 0.5865384340286255\n",
      "Epoch 1/10\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.6396 - accuracy: 0.6826 - val_loss: 32.0428 - val_accuracy: 0.5123\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.5206 - accuracy: 0.7375 - val_loss: 54.4360 - val_accuracy: 0.5123\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 7s 33ms/step - loss: 0.4471 - accuracy: 0.7923 - val_loss: 79.4605 - val_accuracy: 0.5123\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 6s 30ms/step - loss: 0.4260 - accuracy: 0.8063 - val_loss: 96.9728 - val_accuracy: 0.5158\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 8s 36ms/step - loss: 0.4164 - accuracy: 0.8191 - val_loss: 93.0493 - val_accuracy: 0.5158\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.3900 - accuracy: 0.8296 - val_loss: 91.8634 - val_accuracy: 0.5141\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.3854 - accuracy: 0.8372 - val_loss: 75.4645 - val_accuracy: 0.5158\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.3857 - accuracy: 0.8232 - val_loss: 75.8184 - val_accuracy: 0.5158\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.3808 - accuracy: 0.8226 - val_loss: 62.8798 - val_accuracy: 0.5158\n",
      "Epoch 10/10\n",
      "152/215 [====================>.........] - ETA: 1s - loss: 0.3654 - accuracy: 0.8314"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m train_generator_Nep87\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     31\u001b[0m validation_generator_Nep87\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator_Nep87\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator_Nep87\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator_Nep87\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator_Nep87\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator_Nep87\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator_Nep87\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m test_loss_Nep87, test_accuracy_Nep87 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator_Nep87)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNep87 - Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_Nep87\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32md:\\Ky 4\\DPL302m\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# BC15\n",
    "train_generator_BC15.batch_size = 8\n",
    "validation_generator_BC15.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_BC15,\n",
    "    steps_per_epoch=train_generator_BC15.samples // train_generator_BC15.batch_size,\n",
    "    validation_data=validation_generator_BC15,\n",
    "    validation_steps=validation_generator_BC15.samples // validation_generator_BC15.batch_size,\n",
    "    epochs=100\n",
    ")\n",
    "test_loss_BC15, test_accuracy_BC15 = model.evaluate(test_generator_BC15)\n",
    "print(f'BC-15 - Test Loss: {test_loss_BC15}')\n",
    "print(f'BC-15 - Test Accuracy: {test_accuracy_BC15}')\n",
    "\n",
    "# Huongthom\n",
    "train_generator_Huongthom.batch_size = 8\n",
    "validation_generator_Huongthom.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_Huongthom,\n",
    "    steps_per_epoch=train_generator_Huongthom.samples // train_generator_Huongthom.batch_size,\n",
    "    validation_data=validation_generator_Huongthom,\n",
    "    validation_steps=validation_generator_Huongthom.samples // validation_generator_Huongthom.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Huongthom, test_accuracy_Huongthom = model.evaluate(test_generator_Huongthom)\n",
    "print(f'Huongthom - Test Loss: {test_loss_Huongthom}')\n",
    "print(f'Huongthom - Test Accuracy: {test_accuracy_Huongthom}')\n",
    "\n",
    "# Nep87\n",
    "train_generator_Nep87.batch_size = 8\n",
    "validation_generator_Nep87.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_Nep87,\n",
    "    steps_per_epoch=train_generator_Nep87.samples // train_generator_Nep87.batch_size,\n",
    "    validation_data=validation_generator_Nep87,\n",
    "    validation_steps=validation_generator_Nep87.samples // validation_generator_Nep87.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Nep87, test_accuracy_Nep87 = model.evaluate(test_generator_Nep87)\n",
    "print(f'Nep87 - Test Loss: {test_loss_Nep87}')\n",
    "print(f'Nep87 - Test Accuracy: {test_accuracy_Nep87}')\n",
    "\n",
    "# Q5\n",
    "train_generator_Q5.batch_size = 8\n",
    "validation_generator_Q5.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_Q5,\n",
    "    steps_per_epoch=train_generator_Q5.samples // train_generator_Q5.batch_size,\n",
    "    validation_data=validation_generator_Q5,\n",
    "    validation_steps=validation_generator_Q5.samples // validation_generator_Q5.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Q5, test_accuracy_Q5 = model.evaluate(test_generator_Q5)\n",
    "print(f'Q5 - Test Loss: {test_loss_Q5}')\n",
    "print(f'Q5 - Test Accuracy: {test_accuracy_Q5}')\n",
    "\n",
    "# Thien_uu\n",
    "train_generator_Thien_uu.batch_size = 8\n",
    "validation_generator_Thien_uu.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_Thien_uu,\n",
    "    steps_per_epoch=train_generator_Thien_uu.samples // train_generator_Thien_uu.batch_size,\n",
    "    validation_data=validation_generator_Thien_uu,\n",
    "    validation_steps=validation_generator_Thien_uu.samples // validation_generator_Thien_uu.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Thien_uu, test_accuracy_Thien_uu = model.evaluate(test_generator_Thien_uu)\n",
    "print(f'Thien_uu - Test Loss: {test_loss_Thien_uu}')\n",
    "print(f'Thien_uu - Test Accuracy: {test_accuracy_Thien_uu}')\n",
    "\n",
    "# Xi23\n",
    "train_generator_Xi23.batch_size = 8\n",
    "validation_generator_Xi23.batch_size = 8\n",
    "history = model.fit(\n",
    "    train_generator_Xi23,\n",
    "    steps_per_epoch=train_generator_Xi23.samples // train_generator_Xi23.batch_size,\n",
    "    validation_data=validation_generator_Xi23,\n",
    "    validation_steps=validation_generator_Xi23.samples // validation_generator_Xi23.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "test_loss_Xi23, test_accuracy_Xi23 = model.evaluate(test_generator_Xi23)\n",
    "print(f'Xi23 - Test Loss: {test_loss_Xi23}')\n",
    "print(f'Xi23 - Test Accuracy: {test_accuracy_Xi23}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
